/*
 * ChatCLI - Command Line Interface for LLM interaction
 * Copyright (c) 2024 Edilson Freitas
 * License: MIT
 */
package cli

import (
	"context"
	"errors"
	"fmt"
	"os"
	"os/exec"
	"path/filepath"
	"regexp"
	"runtime"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"

	"github.com/c-bata/go-prompt"
	"github.com/diillson/chatcli/config"
	"github.com/diillson/chatcli/llm/catalog"
	"github.com/diillson/chatcli/llm/client"
	"github.com/diillson/chatcli/llm/manager"
	"github.com/diillson/chatcli/llm/openai_assistant"
	"github.com/diillson/chatcli/version"
	"github.com/joho/godotenv"

	"github.com/diillson/chatcli/models"
	"github.com/diillson/chatcli/utils"

	"github.com/charmbracelet/glamour"
	"go.uber.org/zap"
)

// Logger interface para facilitar a testabilidade
type Logger interface {
	Info(msg string, fields ...zap.Field)
	Error(msg string, fields ...zap.Field)
	Warn(msg string, fields ...zap.Field)
	Sync() error
}

// FileChunk representa um peda√ßo do conte√∫do de arquivos
type FileChunk struct {
	Index   int
	Total   int
	Content string
}

type InteractionState int

const (
	StateNormal InteractionState = iota
	StateSwitchingProvider
	StateProcessing
	StateAgentMode
)

var agentModeRequest = errors.New("request to enter agent mode")
var errExitRequest = errors.New("request to exit")
var commandFlags = map[string]map[string][]prompt.Suggest{
	"@file": {
		"--mode": {
			{Text: "full", Description: "Processa o conte√∫do completo (padr√£o, trunca se necess√°rio)"},
			{Text: "summary", Description: "Gera resumo estrutural (√°rvore de arquivos, tamanhos, sem conte√∫do)"},
			{Text: "chunked", Description: "Divide grandes projetos em peda√ßos gerenci√°veis (use /nextchunk para prosseguir)"},
			{Text: "smart", Description: "Seleciona arquivos relevantes com base no seu prompt (IA decide)"},
		},
	},
	"@command": {
		"-i":   {},
		"--ai": {},
	},
	"/switch": {
		"--model":      {},
		"--max-tokens": {},
		"--slugname":   {},
		"--tenantname": {},
	},
	"/session": {
		"new":    {},
		"save":   {},
		"load":   {},
		"list":   {},
		"delete": {},
	},
}

// ChatCLI representa a interface de linha de comando do chat
type ChatCLI struct {
	Client               client.LLMClient
	manager              manager.LLMManager
	logger               *zap.Logger
	Provider             string
	Model                string
	history              []models.Message
	commandHistory       []string
	newCommandsInSession []string
	historyManager       *HistoryManager
	animation            *AnimationManager
	commandHandler       *CommandHandler
	lastCommandOutput    string
	fileChunks           []FileChunk // Chunks pendentes para processamento
	failedChunks         []FileChunk // Chunks que falharam no processamento
	lastFailedChunk      *FileChunk  // Refer√™ncia ao √∫ltimo chunk que falhou
	agentMode            *AgentMode  // Modo de agente
	interactionState     InteractionState
	mu                   sync.Mutex
	operationCancel      context.CancelFunc
	isExecuting          atomic.Bool
	processingDone       chan struct{}
	sessionManager       *SessionManager
	currentSessionName   string
	MaxTokensOverride    int
	UserMaxTokens        int
}

// reconfigureLogger reconfigura o logger ap√≥s o reload das vari√°veis de ambiente
func (cli *ChatCLI) reconfigureLogger() {
	cli.logger.Info("Reconfigurando o logger...")

	// Sincronizar e fechar o logger atual
	if err := cli.logger.Sync(); err != nil {
		cli.logger.Error("Erro ao sincronizar o logger", zap.Error(err))
	}

	// Recriar o logger com base no novo valor da vari√°vel ENV
	newLogger, err := utils.InitializeLogger()
	if err != nil {
		cli.logger.Error("Erro ao reinicializar o logger", zap.Error(err))
		return
	}

	cli.logger = newLogger
	cli.logger.Info("Logger reconfigurado com sucesso")
}

// reloadConfiguration recarrega as vari√°veis de ambiente e reconfigura o LLMManager
func (cli *ChatCLI) reloadConfiguration() {
	fmt.Println("Recarregando configura√ß√µes...")

	// Preservar provider/model atuais do runtime
	prevProvider := cli.Provider
	prevModel := cli.Model

	// Determinar o arquivo .env (mesma l√≥gica do main.go) e expandir caminho se necess√°rio
	envFilePath := os.Getenv("CHATCLI_DOTENV")
	if envFilePath == "" {
		envFilePath = ".env"
	} else {
		if expanded, err := utils.ExpandPath(envFilePath); err == nil {
			envFilePath = expanded
		} else {
			fmt.Printf("Aviso: n√£o foi poss√≠vel expandir o caminho '%s': %v\n", envFilePath, err)
		}
	}
	// Limpar vari√°veis de ambiente relevantes para garantir reload consistente
	variablesToUnset := []string{
		// Gerais
		"LOG_LEVEL", "ENV", "LLM_PROVIDER", "LOG_FILE", "LOG_MAX_SIZE", "HISTORY_MAX_SIZE",
		// OpenAI
		"OPENAI_API_KEY", "OPENAI_MODEL", "OPENAI_ASSISTANT_MODEL",
		"OPENAI_USE_RESPONSES", "OPENAI_MAX_TOKENS",
		// ClaudeAI
		"CLAUDEAI_API_KEY", "CLAUDEAI_MODEL", "CLAUDEAI_MAX_TOKENS", "CLAUDEAI_API_VERSION",
		// Google AI (Gemini)
		"GOOGLEAI_API_KEY", "GOOGLEAI_MODEL", "GOOGLEAI_MAX_TOKENS",
		// StackSpot
		"CLIENT_ID", "CLIENT_SECRET", "SLUG_NAME", "TENANT_NAME",
	}

	for _, variable := range variablesToUnset {
		_ = os.Unsetenv(variable)
	}

	// Recarregar o arquivo .env sobrescrevendo valores (garante atualiza√ß√£o mesmo se j√° havia env setado)
	err := godotenv.Overload(envFilePath)
	if err != nil && !os.IsNotExist(err) {
		cli.logger.Error("Erro ao carregar o arquivo .env", zap.Error(err))
	}

	cli.reconfigureLogger()

	// Recarregar a configura√ß√£o do LLMManager
	utils.CheckEnvVariables(cli.logger)

	manager, err := manager.NewLLMManager(cli.logger, os.Getenv("SLUG_NAME"), os.Getenv("TENANT_NAME"))
	if err != nil {
		cli.logger.Error("Erro ao reconfigurar o LLMManager", zap.Error(err))
		return
	}

	cli.manager = manager

	// Tentar reaproveitar o provider/model escolhidos pelo usu√°rio
	if prevProvider != "" && prevModel != "" {
		if client, err := cli.manager.GetClient(prevProvider, prevModel); err == nil {
			cli.Client = client
			cli.Provider = prevProvider
			cli.Model = prevModel
			fmt.Println("Configura√ß√µes recarregadas com sucesso! (preservado provider/model atuais)")
			return
		}
		// Se falhar (ex.: provider indispon√≠vel), ca√≠mos para o comportamento padr√£o
		cli.logger.Warn("Falha ao preservar provider/model ap√≥s reload; caindo para valores do .env",
			zap.String("provider", prevProvider), zap.String("model", prevModel))
	}
	// Fallback: usar valores do .env
	cli.configureProviderAndModel()
	if client, err := cli.manager.GetClient(cli.Provider, cli.Model); err == nil {
		cli.Client = client
		fmt.Println("Configura√ß√µes recarregadas com sucesso!")
	} else {
		cli.logger.Error("Erro ao obter o cliente LLM", zap.Error(err))
		fmt.Println("Falha ao reconfigurar cliente LLM ap√≥s reload.")
	}
}

func (cli *ChatCLI) configureProviderAndModel() {
	cli.Provider = os.Getenv("LLM_PROVIDER")
	if cli.Provider == "" {
		cli.Provider = config.DefaultLLMProvider
	}
	if cli.Provider == "OPENAI" {
		cli.Model = os.Getenv("OPENAI_MODEL")
		if cli.Model == "" {
			cli.Model = config.DefaultOpenAIModel
		}
	}
	if cli.Provider == "OPENAI_ASSISTANT" {
		cli.Model = os.Getenv("OPENAI_ASSISTANT_MODEL")
		if cli.Model == "" {
			cli.Model = utils.GetEnvOrDefault("OPENAI_MODEL", config.DefaultOpenAiAssistModel) // se n√£o houver, usa o mesmo dos completions ou seta default
		}
	}
	if cli.Provider == "CLAUDEAI" {
		cli.Model = os.Getenv("CLAUDEAI_MODEL")
		if cli.Model == "" {
			cli.Model = config.DefaultClaudeAIModel
		}
	}
	if cli.Provider == "GOOGLEAI" {
		cli.Model = os.Getenv("GOOGLEAI_MODEL")
		if cli.Model == "" {
			cli.Model = config.DefaultGoogleAIModel
		}
	}
	if cli.Provider == "XAI" {
		cli.Model = os.Getenv("XAI_MODEL")
		if cli.Model == "" {
			cli.Model = config.DefaultXAIModel
		}
	}
	if cli.Provider == "OLLAMA" {
		cli.Model = os.Getenv("OLLAMA_MODEL")
		if cli.Model == "" {
			cli.Model = config.DefaultOllamaModel
		}
	}
}

// NewChatCLI cria uma nova inst√¢ncia de ChatCLI
func NewChatCLI(manager manager.LLMManager, logger *zap.Logger) (*ChatCLI, error) {
	cli := &ChatCLI{
		manager:           manager,
		logger:            logger,
		history:           make([]models.Message, 0),
		historyManager:    NewHistoryManager(logger),
		animation:         NewAnimationManager(),
		interactionState:  StateNormal,
		processingDone:    make(chan struct{}),
		MaxTokensOverride: 0,
	}

	cli.configureProviderAndModel()

	client, err := manager.GetClient(cli.Provider, cli.Model)
	if err != nil {
		logger.Error("Erro ao obter o cliente LLM", zap.Error(err))
		return nil, err
	}

	sessionMgr, err := NewSessionManager(logger)
	if err != nil {
		return nil, fmt.Errorf("erro ao inicializar o SessionManager: %w", err)
	}
	cli.sessionManager = sessionMgr
	cli.currentSessionName = ""

	cli.Client = client
	cli.commandHandler = NewCommandHandler(cli)
	cli.agentMode = NewAgentMode(cli, logger)

	history, err := cli.historyManager.LoadHistory()
	if err != nil {
		cli.logger.Error("Erro ao carregar o hist√≥rico", zap.Error(err))
	} else {
		cli.commandHistory = history
	}

	return cli, nil
}

func (cli *ChatCLI) executor(in string) {
	// N√£o resetar a flag aqui. O wrapper controla isso.
	// cli.shouldEnterAgentMode = false

	in = strings.TrimSpace(in)
	if in != "" {
		cli.commandHistory = append(cli.commandHistory, in)
		cli.newCommandsInSession = append(cli.newCommandsInSession, in)
	}

	// Se o comando for para o agente, apenas defina a flag e retorne.
	if strings.HasPrefix(in, "/agent") || strings.HasPrefix(in, "/run") {
		panic(agentModeRequest)
	}

	// Se a entrada estiver vazia, n√£o fa√ßa nada.
	if in == "" {
		return
	}

	// Lida com a sele√ß√£o de provedor
	if cli.interactionState == StateSwitchingProvider {
		cli.handleProviderSelection(in)
		cli.interactionState = StateNormal
		return
	}

	// Comandos especiais
	if strings.Contains(strings.ToLower(in), "@command ") {
		command := strings.TrimPrefix(in, "@command ")
		cli.executeDirectCommand(command)
		return
	}

	if strings.HasPrefix(in, "/") || in == "exit" || in == "quit" {
		if cli.commandHandler.HandleCommand(in) {
			panic(errExitRequest)
		}
		return
	}

	// Para prompts LLM, processar de forma ass√≠ncrona
	cli.interactionState = StateProcessing
	go cli.processLLMRequest(in)
}

// IsExecuting retorna true se uma opera√ß√£o est√° em andamento
func (cli *ChatCLI) IsExecuting() bool {
	return cli.isExecuting.Load()
}

// CancelOperation cancela a opera√ß√£o atual se houver uma
func (cli *ChatCLI) CancelOperation() {
	cli.mu.Lock()
	defer cli.mu.Unlock()

	if cli.operationCancel != nil {
		cli.operationCancel()
	}
}

func (cli *ChatCLI) processLLMRequest(in string) {
	cli.isExecuting.Store(true)

	defer func() {
		cli.isExecuting.Store(false)
		cli.interactionState = StateNormal
		cli.forceRefreshPrompt()
	}()

	ctx, cancel := context.WithCancel(context.Background())
	cli.mu.Lock()
	cli.operationCancel = cancel
	cli.mu.Unlock()

	defer func() {
		cli.mu.Lock()
		cli.operationCancel = nil
		cli.mu.Unlock()
		cancel()
	}()

	fmt.Println()
	cli.animation.ShowThinkingAnimation(cli.Client.GetModelName())

	userInput, additionalContext := cli.processSpecialCommands(in)
	cli.history = append(cli.history, models.Message{
		Role:    "user",
		Content: userInput + additionalContext,
	})

	effectiveMaxTokens := cli.getMaxTokensForCurrentLLM()

	aiResponse, err := cli.Client.SendPrompt(ctx, userInput+additionalContext, cli.history, effectiveMaxTokens)

	cli.animation.StopThinkingAnimation()

	//resetTerminal(cli.logger)

	if err != nil {
		if errors.Is(err, context.Canceled) {
			fmt.Println("üõë Opera√ß√£o cancelada com sucesso!")
			if len(cli.history) > 0 && cli.history[len(cli.history)-1].Role == "user" {
				cli.history = cli.history[:len(cli.history)-1]
			}
		} else {
			// Simplificado para sempre mostrar o erro real vindo do cliente.
			// O erro j√° deve vir sanitizado se necess√°rio.
			fmt.Printf("‚ùå Erro: %s\n", err.Error())
		}
	} else {
		cli.history = append(cli.history, models.Message{
			Role:    "assistant",
			Content: aiResponse,
		})
		modelName := cli.Client.GetModelName()
		coloredPrefix := colorize(modelName+":", ColorPurple)
		renderedResponse := cli.renderMarkdown(aiResponse)
		fmt.Printf("%s ", coloredPrefix)
		cli.typewriterEffect(renderedResponse, 2*time.Millisecond)
		fmt.Println()
	}
}

func (cli *ChatCLI) handleProviderSelection(in string) {
	availableProviders := cli.manager.GetAvailableProviders()
	choiceIndex, err := strconv.Atoi(in)
	if err != nil || choiceIndex < 1 || choiceIndex > len(availableProviders) {
		fmt.Println("Escolha inv√°lida. Voltando ao modo normal.")
		return
	}

	newProvider := availableProviders[choiceIndex-1]
	var newModel string
	if newProvider == "OPENAI" {
		newModel = utils.GetEnvOrDefault("OPENAI_MODEL", config.DefaultOpenAIModel)
	}
	if newProvider == "CLAUDEAI" {
		newModel = utils.GetEnvOrDefault("CLAUDEAI_MODEL", config.DefaultClaudeAIModel)
	}
	if newProvider == "OPENAI_ASSISTANT" {
		newModel = utils.GetEnvOrDefault("OPENAI_ASSISTANT_MODEL", utils.GetEnvOrDefault("OPENAI_MODEL", config.DefaultOpenAiAssistModel))
	}
	if newProvider == "GOOGLEAI" {
		newModel = utils.GetEnvOrDefault("GOOGLEAI_MODEL", config.DefaultGoogleAIModel)
	}
	if newProvider == "XAI" {
		newModel = utils.GetEnvOrDefault("XAI_MODEL", config.DefaultXAIModel)
	}
	if newProvider == "OLLAMA" {
		newModel = utils.GetEnvOrDefault("OLLAMA_MODEL", config.DefaultOllamaModel)
	}

	newClient, err := cli.manager.GetClient(newProvider, newModel)
	if err != nil {
		cli.logger.Error("Erro ao trocar de provedor", zap.Error(err))
		fmt.Println("Erro ao trocar de provedor.")
		return
	}

	cli.Client = newClient
	cli.Provider = newProvider
	cli.Model = newModel
	fmt.Printf("Trocado para %s (%s)\n\n", cli.Client.GetModelName(), cli.Provider)
}

func (cli *ChatCLI) Start(ctx context.Context) {
	defer cli.cleanup()
	cli.PrintWelcomeScreen()

	shouldContinue := true
	for shouldContinue {
		func() {
			defer func() {
				if r := recover(); r != nil {
					// Verificamos qual tipo de p√¢nico ocorreu
					if r == agentModeRequest {
						// Mant√©m shouldContinue = true para entrar no modo agente
					} else if r == errExitRequest {
						// Se for um pedido de sa√≠da, definimos shouldContinue como false
						shouldContinue = false
					} else {
						// Se for outro p√¢nico, relan√ßamos
						panic(r)
					}
				}
			}()

			p := prompt.New(
				cli.executor,
				cli.completer,
				prompt.OptionTitle("ChatCLI - LLM no seu Terminal"),
				prompt.OptionLivePrefix(cli.changeLivePrefix),
				prompt.OptionPrefixTextColor(prompt.Green),
				prompt.OptionInputTextColor(prompt.White),
				prompt.OptionSuggestionBGColor(prompt.DarkGray),
				prompt.OptionDescriptionBGColor(prompt.Black),
				prompt.OptionSuggestionTextColor(prompt.White),
				prompt.OptionDescriptionTextColor(prompt.Yellow),
				prompt.OptionSelectedSuggestionBGColor(prompt.Blue),
				prompt.OptionSelectedDescriptionBGColor(prompt.DarkGray),
				prompt.OptionHistory(cli.commandHistory),
				prompt.OptionMaxSuggestion(10),
				prompt.OptionAddKeyBind(prompt.KeyBind{
					Key: prompt.ControlC,
					Fn:  cli.handleCtrlC,
				}),
			)

			p.Run()
			shouldContinue = false
		}()

		// Se o loop 'for' deve continuar, √© porque o modo agente foi solicitado.
		if shouldContinue {
			cli.restoreTerminal()
			cli.runAgentLogic()
		}
	}
}

// restoreTerminal executa `stty sane` para restaurar o estado do terminal
// para o modo normal ap√≥s o go-prompt deix√°-lo em "raw mode".
// Isso √© necess√°rio em sistemas n√£o-Windows.
func (cli *ChatCLI) restoreTerminal() {
	if runtime.GOOS == "windows" {
		return // stty n√£o est√° dispon√≠vel no Windows
	}
	cmd := exec.Command("stty", "sane")
	cmd.Stdin = os.Stdin // Garante que o comando opere no terminal correto
	if err := cmd.Run(); err != nil {
		cli.logger.Warn("Falha ao restaurar o terminal com 'stty sane'", zap.Error(err))
	}
}

func (cli *ChatCLI) runAgentLogic() {
	// O √∫ltimo comando no hist√≥rico √© o que ativou o agente
	if len(cli.commandHistory) == 0 {
		return
	}
	lastCommand := cli.commandHistory[len(cli.commandHistory)-1]

	// Extrair a consulta
	query := ""
	if strings.HasPrefix(lastCommand, "/agent") {
		query = strings.TrimSpace(strings.TrimPrefix(lastCommand, "/agent"))
	} else if strings.HasPrefix(lastCommand, "/run") {
		query = strings.TrimSpace(strings.TrimPrefix(lastCommand, "/run"))
	} else {
		fmt.Println("Erro: n√£o foi poss√≠vel extrair a consulta do agente.")
		return
	}

	fmt.Printf("\nü§ñ Entrando no modo agente com a consulta: \"%s\"\n", query)
	fmt.Println("O agente analisar√° sua solicita√ß√£o e sugerir√° comandos para resolver.")

	ctx, cancel := context.WithTimeout(context.Background(), 30*time.Minute)
	defer cancel()

	query, additionalContext := cli.processSpecialCommands(query)

	if cli.agentMode == nil {
		cli.agentMode = NewAgentMode(cli, cli.logger)
	}

	err := cli.agentMode.Run(ctx, query, additionalContext)
	if err != nil {
		fmt.Printf(" ‚ùå Erro no modo agente: %v\n", err)
	}

	fmt.Println("\n ‚úÖ Retornando ao chat...")
	time.Sleep(1 * time.Second)
}

func (cli *ChatCLI) handleCtrlC(buf *prompt.Buffer) {
	if cli.isExecuting.Load() {
		fmt.Println("\n‚ö†Ô∏è Ctrl + C acionado - Cancelando opera√ß√£o...")

		cli.mu.Lock()
		if cli.operationCancel != nil {
			cli.operationCancel()
		}
		cli.mu.Unlock()

		// For√ßar volta ao estado normal
		cli.interactionState = StateNormal

		// reset
		//resetTerminal(cli.logger)

		cli.forceRefreshPrompt()

	} else {
		fmt.Println("\nAt√© breve!! CTRL + C Duplo...")
		cli.cleanup()
		os.Exit(0)
	}
}

func (cli *ChatCLI) changeLivePrefix() (string, bool) {
	switch cli.interactionState {
	case StateSwitchingProvider:
		return "Escolha o provedor (pelo n√∫mero): ", true
	case StateProcessing, StateAgentMode:
		return "", true
	default:
		// Mostra o nome da sess√£o no prompt
		if cli.currentSessionName != "" {
			return fmt.Sprintf("%s ‚ùØ ", cli.currentSessionName), true // <--- CORRIGIDO: Retorna texto puro
		}
		return "‚ùØ ", true
	}
}

// cleanup realiza a limpeza de recursos ao encerrar o ChatCLI
func (cli *ChatCLI) cleanup() {
	if err := cli.historyManager.AppendAndRotateHistory(cli.newCommandsInSession); err != nil {
		cli.logger.Error("Erro ao salvar hist√≥rico", zap.Error(err))
	}
	if assistantClient, ok := cli.Client.(*openai_assistant.OpenAIAssistantClient); ok {
		if err := assistantClient.Cleanup(); err != nil {
			cli.logger.Error("Erro na limpeza do OpenAI Assistant", zap.Error(err))
		}
	}
	if err := cli.logger.Sync(); err != nil {
		fmt.Printf("Falha ao sincronizar logger: %v\n", err)
	}
}

// handleSwitchCommand Processa os comando na entrada para atualizar o slug/tenant, ou mudar o modelo para LLM atual.
func (cli *ChatCLI) handleSwitchCommand(userInput string) {
	args := strings.Fields(userInput)
	var newSlugName, newTenantName, newModel string
	shouldUpdateToken := false
	shouldSwitchModel := false
	maxTokensOverride := -1

	for i := 1; i < len(args); i++ {
		switch args[i] {
		case "--slugname":
			if i+1 < len(args) {
				newSlugName = args[i+1]
				shouldUpdateToken = true
				i++ // Pular o valor
			}
		case "--tenantname":
			if i+1 < len(args) {
				newTenantName = args[i+1]
				shouldUpdateToken = true
				i++ // Pular o valor
			}
		case "--model":
			if i+1 < len(args) {
				newModel = args[i+1]
				shouldSwitchModel = true
				i++ // Pular o valor
			}
		case "--max-tokens":
			if i+1 < len(args) {
				val, err := strconv.Atoi(args[i+1])
				if err == nil && val >= 0 {
					maxTokensOverride = val
				} else {
					fmt.Printf(" ‚ùå Valor inv√°lido para --max-tokens: '%s'. Deve ser um n√∫mero >= 0.\n", args[i+1])
				}
				i++
			}
		}
	}
	if maxTokensOverride != -1 {
		cli.UserMaxTokens = maxTokensOverride
		fmt.Printf(" ‚úÖ Limite m√°ximo de tokens definido para: %d (0 = usar padr√£o do provedor)\n", cli.UserMaxTokens)
	}

	if newSlugName != "" || newTenantName != "" {
		tokenManager, ok := cli.manager.GetTokenManager()
		if ok {
			currentSlugName, currentTenantName := tokenManager.GetSlugAndTenantName()
			if newSlugName != "" {
				fmt.Printf("Atualizando slugName de '%s' para '%s'\n", currentSlugName, newSlugName)
				currentSlugName = newSlugName
			}
			if newTenantName != "" {
				fmt.Printf("Atualizando tenantName de '%s' para '%s'\n", currentTenantName, newTenantName)
				currentTenantName = newTenantName
			}
			tokenManager.SetSlugAndTenantName(currentSlugName, currentTenantName)

			if shouldUpdateToken {
				fmt.Println("Atualizando token com os novos valores...")
				_, err := tokenManager.RefreshToken(context.Background())
				if err != nil {
					cli.logger.Error("Erro ao atualizar o token", zap.Error(err))
				} else {
					fmt.Println("Token atualizado com sucesso!")
				}
			}
		} else {
			fmt.Println("TokenManager n√£o configurado. O provedor STACKSPOT n√£o est√° dispon√≠vel.")
		}
	}

	if shouldSwitchModel {
		fmt.Printf("Tentando trocar para o modelo '%s' no provedor '%s'...\n", newModel, cli.Provider)
		newClient, err := cli.manager.GetClient(cli.Provider, newModel)
		if err != nil {
			fmt.Printf(" ‚ùå Erro ao trocar para o modelo '%s': %v\n", newModel, err)
			fmt.Println("   Verifique se o nome do modelo est√° correto para o provedor atual.")
		} else {
			cli.Client = newClient
			cli.Model = newModel // Atualiza o modelo no estado do CLI
			fmt.Printf(" ‚úÖ Modelo trocado com sucesso para %s (%s)\n", cli.Client.GetModelName(), cli.Provider)
		}
		return // Finaliza ap√≥s a tentativa de troca de modelo
	}

	if !shouldUpdateToken && !shouldSwitchModel && maxTokensOverride == -1 && len(args) == 1 {
		cli.switchProvider()
	}
}

func (cli *ChatCLI) switchProvider() {
	fmt.Println("Provedores dispon√≠veis:")
	availableProviders := cli.manager.GetAvailableProviders()
	for i, provider := range availableProviders {
		fmt.Printf("%d. %s\n", i+1, provider)
	}
	cli.interactionState = StateSwitchingProvider
}

func (cli *ChatCLI) showHelp() {
	// Helper para formatar linhas com alinhamento
	printCommand := func(cmd, desc string) {
		cmdColor := ColorCyan
		descColor := ColorGray
		// Deixa a descri√ß√£o em branco se o comando for um exemplo
		if strings.HasPrefix(cmd, "  ") {
			cmdColor = ColorGray
			descColor = ColorGray
		}
		fmt.Printf("    %s    %s\n", colorize(fmt.Sprintf("%-32s", cmd), cmdColor), colorize(desc, descColor))
	}

	// Cabe√ßalho
	fmt.Println("\n" + colorize(ColorBold, "Guia Completo de Comandos do ChatCLI"))
	fmt.Println(colorize("Aqui est√° um mapa exaustivo de todos os comandos, subcomandos, flags e op√ß√µes dispon√≠veis.", ColorGray))
	fmt.Println(colorize("Use-os para controlar a aplica√ß√£o, adicionar contexto ou automatizar tarefas.", ColorGray))

	// --- Controle Geral da Aplica√ß√£o ---
	fmt.Printf("\n  %s\n", colorize("Controle Geral", ColorLime))
	printCommand("/help", "Mostra esta tela de ajuda completa.")
	printCommand("/exit | /quit", "Encerra a aplica√ß√£o.")
	printCommand("/newsession", "Limpa o hist√≥rico da conversa atual e inicia uma nova sess√£o (alias de /session new).")
	printCommand("/version | /v", "Mostra a vers√£o, commit hash, data de build e verifica atualiza√ß√µes.")

	// --- Configura√ß√£o e Provedores de IA ---
	fmt.Printf("\n  %s\n", colorize("Configura√ß√£o e Provedores de IA", ColorLime))
	printCommand("/switch", "Abre o menu interativo para trocar o provedor de LLM (ex.: OPENAI, CLAUDEAI).")
	printCommand("/switch --model <nome>", "Muda o modelo do provedor atual (ex.: gpt-4o-mini, grok-4, gpt5... etc).")
	printCommand("/switch --max-tokens <num>", "Define o m√°ximo de tokens para as pr√≥ximas respostas (0 para padr√£o).")
	printCommand("  Ex: /switch --model gpt-4o-mini", "(Muda para o modelo GPT-4o Mini na OpenAI)")
	printCommand("/switch --slugname <slug>", "Atualiza o 'slugName' (apenas para StackSpot).")
	printCommand("/switch --tenantname <tenant>", "Atualiza o 'tenantName' (apenas para StackSpot).")
	printCommand("/config | /status", "Exibe a configura√ß√£o atual (provedor, modelo, chaves, etc.).")
	printCommand("/reload", "Recarrega as vari√°veis ou configura√ß√µes do seu arquivo .env em tempo real.")

	// --- Comandos de Contexto (usados em prompts) ---
	fmt.Printf("\n  %s\n", colorize("Adicionando Contexto aos Prompts", ColorLime))
	printCommand("@file <caminho>", "Adiciona o conte√∫do de um arquivo ou diret√≥rio ao prompt.")
	printCommand("  --mode full", "(Padr√£o) Envia o conte√∫do completo, truncando se necess√°rio.")
	printCommand("  --mode chunked", "Para projetos grandes, divide em peda√ßos (chunks).")
	printCommand("  --mode summary", "Envia apenas a estrutura de arquivos, sem o conte√∫do.")
	printCommand("  --mode smart", "A IA seleciona os arquivos mais relevantes para sua pergunta.")
	printCommand("  Ex: @file --mode=smart ./src Como funciona o login?", "")
	printCommand("@git", "Adiciona status, branch, remotos e commits recentes do Git.")
	printCommand("@history", "Adiciona os √∫ltimos comandos do seu hist√≥rico de shell (bash/zsh/fish).")
	printCommand("@env", "Adiciona as vari√°veis de ambiente (valores sens√≠veis s√£o ocultados).")

	// --- Gerenciamento de Chunks (para @file --mode chunked) ---
	fmt.Printf("\n  %s\n", colorize("Gerenciamento de Arquivos Grandes (Chunks)", ColorLime))
	printCommand("/nextchunk", "Envia o pr√≥ximo peda√ßo (chunk) do projeto para a IA.")
	printCommand("/retry", "Tenta reenviar o √∫ltimo chunk que falhou.")
	printCommand("/retryall", "Tenta reenviar todos os chunks que falharam.")
	printCommand("/skipchunk", "Pula um chunk com erro e continua para o pr√≥ximo.")

	// --- Execu√ß√£o de Comandos (@command) ---
	fmt.Printf("\n  %s\n", colorize("Execu√ß√£o de Comandos no Terminal", ColorLime))
	printCommand("@command <cmd>", "Executa um comando e adiciona sua sa√≠da ao prompt.")
	printCommand("  Ex: @command ls -la", "(Executa 'ls -la' e anexa o resultado)")
	printCommand("@command -i <cmd>", "Executa um comando interativo (ex.: vim) sem adicionar sa√≠da ao prompt.")
	printCommand("@command --ai <cmd>", "Executa um comando e envia a sa√≠da DIRETAMENTE para a IA.")
	printCommand("  Ex: @command --ai git diff", "(Envia as diferen√ßas do git para an√°lise da IA)")
	printCommand("@command --ai <cmd> > <texto>", "Igual ao anterior, mas adiciona um contexto/pergunta.")
	printCommand("  Ex: @command --ai cat err.log > resuma este erro", "")

	// --- Modo Agente (Automa√ß√£o de Tarefas) ---
	fmt.Printf("\n  %s\n", colorize("Modo Agente (Execu√ß√£o de Tarefas)", ColorLime))
	printCommand("/agent <tarefa>", "Pede √† IA para planejar e executar comandos para resolver uma tarefa.")
	printCommand("/run <tarefa>", "Um atalho (alias) para o comando /agent.")
	printCommand("  Ex: /agent liste todos os arquivos .go e conte suas linhas", "")
	printCommand("Dentro do modo agente:", "")
	printCommand("  [1..N]", "Executa um comando espec√≠fico (ex: 1 para o primeiro).")
	printCommand("  a", "Executa TODOS os comandos sugeridos em sequ√™ncia.")
	printCommand("  eN", "Edita o comando N antes de executar (ex: e1).")
	printCommand("  tN", "Simula (dry-run) o comando N (ex: t2).")
	printCommand("  cN", "Pede continua√ß√£o √† IA com a sa√≠da do comando N (ex: c2).")
	printCommand("  pcN", "Adiciona contexto ao comando N ANTES de executar (ex: pc1).")
	printCommand("  acN", "Adiciona contexto √† SA√çDA do comando N (ex: ac1).")
	printCommand("  vN", "Abre a sa√≠da completa do comando N no pager (less -R/more).")
	printCommand("  wN", "Salva a sa√≠da do comando N em um arquivo tempor√°rio.")
	printCommand("  p", "Alterna a visualiza√ß√£o do plano: COMPACTO ‚Üî COMPLETO.")
	printCommand("  r", "Atualiza a tela (clear + redraw).")
	printCommand("  q", "Sai do modo agente.")
	printCommand("Observa√ß√µes:", "")
	printCommand("  ‚Ä¢ √öltimo Resultado", "sempre ancorado ao rodap√© da tela (preview).")
	printCommand("  ‚Ä¢ Plano COMPACTO", "mostra 1 linha por comando (status + descri√ß√£o + 1¬™ linha do c√≥digo).")
	printCommand("  ‚Ä¢ Plano COMPLETO", "mostra cart√£o com descri√ß√£o, tipo, risco e bloco de c√≥digo formatado.")

	// --- Gerenciamento de Sess√µes (/session) ---
	fmt.Printf("\n  %s\n", colorize("Gerenciamento de Sess√µes", ColorLime))
	printCommand("/session save <nome>", "Salva a sess√£o atual com um nome (ex.: /session save minha-conversa).")
	printCommand("/session load <nome>", "Carrega uma sess√£o salva (ex.: /session load minha-conversa).")
	printCommand("/session list", "Lista todas as sess√µes salvas.")
	printCommand("/session delete <nome>", "Deleta uma sess√£o salva (ex.: /session delete minha-conversa).")
	printCommand("/session new", "Inicia uma nova sess√£o limpa (alias de /newsession).")

	// --- Modo N√£o-Interativo (One-Shot) ---
	fmt.Printf("\n  %s\n", colorize("Modo N√£o-Interativo (One-Shot, para scripts e pipes)", ColorLime))
	printCommand("chatcli -p \"<prompt>\"", "Executa um prompt uma √∫nica vez e sai.")
	printCommand("  Ex: chatcli -p \"Explique este reposit√≥rio.\"", "")
	printCommand("chatcli --prompt \"<prompt>\"", "Alias de -p.")
	printCommand("--provider <nome>", "Override do provedor (ex.: --provider OPENAI).")
	printCommand("--model <nome>", "Override do modelo (ex.: --model gpt-4o-mini).")
	printCommand("--max-tokens <num>", "Override do m√°ximo de tokens para a resposta.")
	printCommand("--timeout <dura√ß√£o>", "Timeout da chamada (ex.: --timeout 5m, padr√£o: 5m).")
	printCommand("--no-anim", "Desabilita anima√ß√µes (√∫til em scripts/CI).")
	printCommand("--agent-auto-exec", "No modo agente one-shot, executa o primeiro comando sugerido automaticamente se for seguro.")
	printCommand("Uso com pipes (stdin):", "Envia dados via pipe (ex.: git diff | chatcli -p \"Resuma as mudan√ßas.\").")

	// --- Dicas de Uso e Atalhos ---
	fmt.Printf("\n  %s\n", colorize("Dicas e Atalhos Gerais", ColorLime))
	printCommand("Cancelamento (Ctrl+C)", "Pressione Ctrl+C uma vez durante o 'Pensando...' para cancelar.")
	printCommand("Sa√≠da R√°pida (Ctrl+D)", "Pressione Ctrl+D no prompt vazio para sair do ChatCLI.")
	printCommand("Operador '>'", "Use '>' para adicionar contexto em prompts (ex.: @git > Crie um release note).")
	printCommand("Modo Agente: p", "Alterna COMPACTO/COMPLETO do plano (√∫til para focar no fluxo).")
	printCommand("Modo Agente: vN", "Abre a sa√≠da do comando N no pager (leitura longa sem poluir a tela).")
	printCommand("Modo Agente: wN", "Salva a sa√≠da do comando N em arquivo tempor√°rio (para compartilhar ou anexar).")
	printCommand("Modo Agente: r", "Redesenha a tela (clear) mantendo o foco no '√öltimo Resultado'.")

	fmt.Println()
}

// ApplyOverrides atualiza provider/model e reobt√©m o client correspondente
func (cli *ChatCLI) ApplyOverrides(mgr manager.LLMManager, provider, model string) error {
	if provider == "" && model == "" {
		return nil
	}
	prov := cli.Provider
	mod := cli.Model
	if provider != "" {
		prov = strings.ToUpper(provider)
	}
	if model != "" {
		mod = model
	}
	if prov == cli.Provider && mod == cli.Model {
		return nil
	}
	newClient, err := mgr.GetClient(prov, mod)
	if err != nil {
		return err
	}
	cli.Client = newClient
	cli.Provider = prov
	cli.Model = mod
	return nil
}

// processSpecialCommands processa comandos especiais como @history, @git, @env, @file
func (cli *ChatCLI) processSpecialCommands(userInput string) (string, string) {
	var additionalContext string

	// Processar comandos especiais
	userInput, context := cli.processHistoryCommand(userInput)
	additionalContext += context

	userInput, context = cli.processGitCommand(userInput)
	additionalContext += context

	userInput, context = cli.processEnvCommand(userInput)
	additionalContext += context

	userInput, context = cli.processFileCommand(userInput)
	additionalContext += context

	// Processar '>' como um operador para adicionar contexto
	if idx := strings.Index(userInput, ">"); idx != -1 {
		additionalContext += userInput[idx+1:] + "\n"
		userInput = userInput[:idx]
	}

	// Remover espa√ßos extras
	userInput = strings.TrimSpace(userInput)

	return userInput, additionalContext
}

func removeCommandAndNormalizeSpaces(userInput, command string) string {
	regexPattern := fmt.Sprintf(`(?i)\s*%s\s*`, regexp.QuoteMeta(command))
	re := regexp.MustCompile(regexPattern)
	userInput = re.ReplaceAllString(userInput, " ")
	userInput = regexp.MustCompile(`\s+`).ReplaceAllString(userInput, " ")
	userInput = strings.TrimSpace(userInput)
	return userInput
}

// processHistoryCommand adiciona o hist√≥rico do shell ao contexto
func (cli *ChatCLI) processHistoryCommand(userInput string) (string, string) {
	var additionalContext string
	if strings.Contains(strings.ToLower(userInput), "@history") {
		historyData, err := utils.GetShellHistory()
		if err != nil {
			cli.logger.Error("Erro ao obter o hist√≥rico do shell", zap.Error(err))
		} else {
			allLines := filterEmptyLines(strings.Split(historyData, "\n"))
			total := len(allLines)

			n := 30
			var lastLines []string
			if total > n {
				lastLines = allLines[total-n:]
			} else {
				lastLines = allLines
			}

			startNumber := total - len(lastLines) + 1
			formatted := make([]string, len(lastLines))
			for i, cmd := range lastLines {
				formatted[i] = fmt.Sprintf("%d: %s", startNumber+i, cmd)
			}
			limitedHistoryData := strings.Join(formatted, "\n")
			additionalContext += "\nHist√≥rico do Shell (√∫ltimos 30 comandos):\n" + limitedHistoryData
		}
		userInput = removeCommandAndNormalizeSpaces(userInput, "@history")
	}
	return userInput, additionalContext
}

// processGitCommand adiciona informa√ß√µes do Git ao contexto
func (cli *ChatCLI) processGitCommand(userInput string) (string, string) {
	var additionalContext string
	if strings.Contains(strings.ToLower(userInput), "@git") {
		executor := utils.NewOSCommandExecutor()
		gitData, err := utils.GetGitInfo(executor)
		if err != nil {
			cli.logger.Error("Erro ao obter informa√ß√µes do Git", zap.Error(err))
		} else {
			additionalContext += "\nInforma√ß√µes do Git:\n" + gitData
		}
		userInput = removeCommandAndNormalizeSpaces(userInput, "@git")
	}
	return userInput, additionalContext
}

// processEnvCommand adiciona as vari√°veis de ambiente ao contexto
func (cli *ChatCLI) processEnvCommand(userInput string) (string, string) {
	var additionalContext string
	if strings.Contains(strings.ToLower(userInput), "@env") {
		envData := utils.GetEnvVariablesSanitized()
		additionalContext += "\nVari√°veis de Ambiente:\n" + envData
		userInput = removeCommandAndNormalizeSpaces(userInput, "@env")
	}
	return userInput, additionalContext
}

// processFileCommand adiciona o conte√∫do de arquivos ou diret√≥rios ao contexto
func (cli *ChatCLI) processFileCommand(userInput string) (string, string) {
	var additionalContext string

	if strings.Contains(strings.ToLower(userInput), "@file") {
		// Removida a verifica√ß√£o especial para o OpenAI Assistant
		// Agora, sempre usamos o mesmo processamento de arquivos, independentemente do modelo

		paths, options, err := extractFileCommandOptions(userInput)
		if err != nil {
			cli.logger.Error("Erro ao processar os comandos @file", zap.Error(err))
			return userInput, fmt.Sprintf("\nErro ao processar @file: %s\n", err.Error())
		}

		// Usar o modo a partir das op√ß√µes j√° extra√≠das
		mode := config.ModeFull // Modo padr√£o
		if modeVal, ok := options["mode"]; ok {
			mode = modeVal
		}

		// Configurar estimador de tokens e obter limite m√°ximo de tokens do LLM atual
		tokenEstimator := cli.getTokenEstimatorForCurrentLLM()
		maxTokens := cli.getMaxTokensForCurrentLLM()

		// Processar cada caminho encontrado ap√≥s @file
		for _, path := range paths {
			// Configura√ß√µes de escaneamento
			scanOptions := utils.DefaultDirectoryScanOptions(cli.logger)

			// 1. Pr√©-escanear para obter a contagem total de arquivos
			totalFiles, err := utils.CountMatchingFiles(path, scanOptions)
			if err != nil {
				cli.logger.Error("Erro ao contar arquivos", zap.String("path", path), zap.Error(err))
				additionalContext += fmt.Sprintf("\nErro ao analisar o diret√≥rio '%s': %s\n", path, err.Error())
				continue
			}

			// 2. Inicializar contador para os arquivos processados
			var processedFiles int32 = 0

			// 3. Definir o callback para atualizar a anima√ß√£o com progresso rico
			scanOptions.OnFileProcessed = func(info utils.FileInfo) {
				// Usar atomic para seguran√ßa em concorr√™ncia, embora o callback seja chamado em s√©rie aqui
				atomic.AddInt32(&processedFiles, 1)
				// Atualiza a mensagem da anima√ß√£o com o progresso
				cli.animation.UpdateMessage(
					fmt.Sprintf("Analisando... [%d/%d] %s", atomic.LoadInt32(&processedFiles), totalFiles, info.Path),
				)
			}

			// Escolher a forma de processar (summary, chunked, smartChunk ou full)
			switch mode {
			case config.ModeSummary:
				// Atualizar a mensagem da anima√ß√£o para o modo summary
				cli.animation.UpdateMessage(fmt.Sprintf("Gerando resumo para %s...", path))
				summary, err := cli.processDirectorySummary(path, tokenEstimator, maxTokens)
				if err != nil {
					additionalContext += fmt.Sprintf("\nErro ao processar '%s': %s\n", path, err.Error())
				} else {
					additionalContext += summary
				}

			case config.ModeChunked:
				// Atualizar a mensagem da anima√ß√£o para o modo chunked
				cli.animation.UpdateMessage(fmt.Sprintf("Dividindo %s em chunks...", path))
				chunks, err := cli.processDirectoryChunked(path, tokenEstimator, maxTokens)
				if err != nil {
					additionalContext += fmt.Sprintf("\nErro ao processar '%s': %s\n", path, err.Error())
				} else {
					// Apenas o primeiro chunk √© adicionado diretamente ao contexto.
					if len(chunks) > 0 {
						totalChunks := len(chunks)
						var totalFiles int
						var totalSize int64

						// Contar estimativa de arquivos e tamanho total
						for _, chunk := range chunks {
							fileCount := strings.Count(chunk.Content, "üìÑ ARQUIVO")
							totalFiles += fileCount
							totalSize += int64(len(chunk.Content))
						}

						chunkSummary := fmt.Sprintf(
							"üìä PROJETO DIVIDIDO EM CHUNKS\n"+
								"=============================\n"+
								"‚ñ∂Ô∏è Total de chunks: %d\n"+
								"‚ñ∂Ô∏è Arquivos estimados: ~%d\n"+
								"‚ñ∂Ô∏è Tamanho total: %.2f MB\n"+
								"‚ñ∂Ô∏è Voc√™ est√° no chunk 1/%d\n"+
								"‚ñ∂Ô∏è Use '/nextchunk' para avan√ßar para o pr√≥ximo chunk\n\n"+
								"=============================\n\n",
							totalChunks, totalFiles, float64(totalSize)/1024/1024, totalChunks,
						)

						// Exibir o resumo no console e aguardar 5 segundos para o usu√°rio ler
						fmt.Println()
						fmt.Println(chunkSummary)
						fmt.Println("Aguarde 5 segundos antes de enviar o primeiro chunk...")

						// Usar timer em vez de aguardar input do usu√°rio
						time.Sleep(5 * time.Second)

						fmt.Println("Enviando primeiro chunk para a LLM...")

						// Agora concatenamos ao contexto o resumo + o primeiro chunk
						additionalContext += chunkSummary + chunks[0].Content

						// Guardar os pr√≥ximos chunks para o /nextchunk
						cli.fileChunks = chunks[1:]

						// Avisar o usu√°rio sobre chunks pendentes
						if len(cli.fileChunks) > 0 {
							additionalContext += fmt.Sprintf(
								"\n\n‚ö†Ô∏è ATEN√á√ÉO: Ainda existem %d chunks adicionais. Use /nextchunk quando terminar de analisar este chunk.\n",
								len(cli.fileChunks),
							)
						}
					}
				}

			case config.ModeSmartChunk:
				// Atualizar a mensagem da anima√ß√£o para o modo smart
				cli.animation.UpdateMessage(fmt.Sprintf("Analisando relev√¢ncia dos arquivos em %s...", path))

				// Extrair a consulta do usu√°rio (tudo o que vier ap√≥s o @file + op√ß√µes)
				query := extractUserQuery(userInput)
				relevantContent, err := cli.processDirectorySmart(path, query, tokenEstimator, maxTokens)
				if err != nil {
					additionalContext += fmt.Sprintf("\nErro ao processar '%s': %s\n", path, err.Error())
				} else {
					additionalContext += relevantContent
				}

			default: // ModeFull - comportamento atual (inclui todo o conte√∫do relevante dentro de um limite)
				// Ajustar limite de tamanho com base em tokens dispon√≠veis
				scanOptions.MaxTotalSize = estimateBytesFromTokens(maxTokens*3/4, tokenEstimator)

				files, err := utils.ProcessDirectory(path, scanOptions)
				if err != nil {
					cli.logger.Error(fmt.Sprintf("Erro ao processar '%s'", path), zap.Error(err))
					additionalContext += fmt.Sprintf("\nErro ao processar '%s': %s\n", path, err.Error())
					continue
				}

				if len(files) == 0 {
					additionalContext += fmt.Sprintf("\nNenhum arquivo relevante encontrado em '%s'\n", path)
					continue
				}

				formattedContent := utils.FormatDirectoryContent(files, scanOptions.MaxTotalSize)
				additionalContext += fmt.Sprintf("\n%s\n", formattedContent)
			}
		}

		// Remover o comando @file do input original para n√£o poluir o prompt final
		userInput = removeAllFileCommands(userInput)
	}

	return userInput, additionalContext
}

// extractFileCommandOptions extrai caminhos e op√ß√µes do comando @file
func extractFileCommandOptions(input string) ([]string, map[string]string, error) {
	var paths []string
	options := make(map[string]string)

	// Regex atualizada para encontrar blocos @file com op√ß√µes e caminho
	// Aceita tanto "--key=value" quanto "--key value"
	re := regexp.MustCompile(`@file((?:\s+--\w+(?:(?:=|\s+)\S+)?)*\s+[\w~/.-]+/?[\w.-]*)`)
	matches := re.FindAllStringSubmatch(input, -1)

	for _, match := range matches {
		if len(match) < 2 {
			continue
		}

		// Divide o bloco de comando em tokens
		tokens := strings.Fields(match[1])
		var currentPath string

		// Itera sobre os tokens para separar op√ß√µes do caminho
		i := 0
		for i < len(tokens) {
			token := tokens[i]
			if strings.HasPrefix(token, "--") {
				key := strings.TrimPrefix(token, "--")
				// Formato --key=value
				if parts := strings.SplitN(key, "=", 2); len(parts) == 2 {
					options[parts[0]] = parts[1]
					i++
					// Formato --key value
				} else if i+1 < len(tokens) && !strings.HasPrefix(tokens[i+1], "--") {
					options[key] = tokens[i+1]
					i += 2 // Pula a chave e o valor
				} else {
					// Op√ß√£o sem valor (flag booleana)
					options[key] = "true"
					i++
				}
			} else {
				// O primeiro token que n√£o √© op√ß√£o √© o caminho do arquivo
				currentPath = token
				break // Para a an√°lise de op√ß√µes para este comando @file
			}
		}
		if currentPath != "" {
			paths = append(paths, currentPath)
		}
	}

	if len(paths) == 0 && len(matches) > 0 {
		return nil, nil, fmt.Errorf("comando @file encontrado, mas nenhum caminho v√°lido foi especificado")
	}

	return paths, options, nil
}

// getTokenEstimatorForCurrentLLM retorna um estimador de tokens para o LLM atual
func (cli *ChatCLI) getTokenEstimatorForCurrentLLM() func(string) int {
	// Fun√ß√£o padr√£o - estimativa conservadora
	return func(text string) int {
		// Aproximadamente 4 caracteres por token para a maioria dos modelos
		return len(text) / 4
	}
}

func (cli *ChatCLI) getMaxTokensForCurrentLLM() int {
	// 1. Prioridade m√°xima para o override do usu√°rio via flag
	if cli.UserMaxTokens > 0 {
		return cli.UserMaxTokens
	}

	// Overrides por ENV t√™m preced√™ncia e d√£o flexibilidade operacional
	var override int
	if strings.ToUpper(cli.Provider) == "OPENAI" {
		if v := os.Getenv("OPENAI_MAX_TOKENS"); v != "" {
			if n, err := strconv.Atoi(v); err == nil && n > 0 {
				override = n
			}
		}
	} else if strings.ToUpper(cli.Provider) == "CLAUDEAI" {
		if v := os.Getenv("CLAUDEAI_MAX_TOKENS"); v != "" {
			if n, err := strconv.Atoi(v); err == nil && n > 0 {
				override = n
			}
		}
	} else if strings.ToUpper(cli.Provider) == "GOOGLEAI" {
		if v := os.Getenv("GOOGLEAI_MAX_TOKENS"); v != "" {
			if n, err := strconv.Atoi(v); err == nil && n > 0 {
				override = n
			}
		}
	} else if strings.ToUpper(cli.Provider) == "XAI" {
		if v := os.Getenv("XAI_MAX_TOKENS"); v != "" {
			if n, err := strconv.Atoi(v); err == nil && n > 0 {
				override = n
			}
		}
	} else if strings.ToUpper(cli.Provider) == "OLLAMA" {
		if v := os.Getenv("OLLAMA_MAX_TOKENS"); v != "" {
			if n, err := strconv.Atoi(v); err == nil && n > 0 {
				override = n
			}
		}
	}
	return catalog.GetMaxTokens(cli.Provider, cli.Model, override)
}

// estimateBytesFromTokens estima a quantidade de bytes baseada em tokens
func estimateBytesFromTokens(tokens int, estimator func(string) int) int64 {
	// Teste com uma string comum para calcular a raz√£o bytes/token
	testString := strings.Repeat("typical code sample with various chars 12345!@#$%", 100)
	tokensInTest := estimator(testString)
	bytesPerToken := float64(len(testString)) / float64(tokensInTest)

	// Retorna bytes estimados com margem de seguran√ßa de 90%
	return int64(float64(tokens) * bytesPerToken * 0.9)
}

// processDirectorySummary gera um resumo estrutural do diret√≥rio sem conte√∫do completo
func (cli *ChatCLI) processDirectorySummary(path string, tokenEstimator func(string) int, maxTokens int) (string, error) {
	path, err := utils.ExpandPath(path)
	if err != nil {
		return "", fmt.Errorf("erro ao expandir o caminho: %w", err)
	}

	fileInfo, err := os.Stat(path)
	if err != nil {
		return "", fmt.Errorf("erro ao acessar o caminho: %w", err)
	}

	// Se for um arquivo √∫nico
	if !fileInfo.IsDir() {
		extension := filepath.Ext(path)
		fileType := utils.DetectFileType(path)
		size := fileInfo.Size()

		return fmt.Sprintf("üìÑ %s (%s, %.2f KB)\nTipo: %s\nTamanho: %d bytes\n",
			path, extension, float64(size)/1024, fileType, size), nil
	}

	// Se for um diret√≥rio, escanear a estrutura
	var builder strings.Builder
	builder.WriteString(fmt.Sprintf("üìÅ ESTRUTURA DO DIRET√ìRIO: %s\n\n", path))

	// Mapeamentos para estat√≠sticas
	fileTypes := make(map[string]int)
	var totalSize int64
	var totalFiles int
	var totalDirs int

	// Fun√ß√£o recursiva para construir √°rvore de diret√≥rios
	var buildTree func(dir string, prefix string, depth int) error
	buildTree = func(dir string, prefix string, depth int) error {
		if depth > 3 { // Limitar profundidade para evitar estruturas gigantes
			builder.WriteString(prefix + "...\n")
			return nil
		}

		entries, err := os.ReadDir(dir)
		if err != nil {
			return err
		}

		for i, entry := range entries {
			// Verificar se estamos dentro do limite de tokens
			if tokenEstimator(builder.String()) > maxTokens/2 {
				builder.WriteString(prefix + "... (truncado por limite de tokens)\n")
				return nil
			}

			isLast := i == len(entries)-1

			var newPrefix string
			if isLast {
				builder.WriteString(prefix + "‚îî‚îÄ‚îÄ ")
				newPrefix = prefix + "    "
			} else {
				builder.WriteString(prefix + "‚îú‚îÄ‚îÄ ")
				newPrefix = prefix + "‚îÇ   "
			}

			entryPath := filepath.Join(dir, entry.Name())

			if entry.IsDir() {
				// Verificar se √© um diret√≥rio que normalmente seria ignorado
				if utils.ShouldSkipDir(entry.Name()) {
					builder.WriteString(entry.Name() + "/ (ignorado)\n")
					continue
				}

				totalDirs++
				builder.WriteString(entry.Name() + "/\n")

				// Recursivamente processar subdiret√≥rios
				err := buildTree(entryPath, newPrefix, depth+1)
				if err != nil {
					return err
				}
			} else {
				totalFiles++
				fInfo, err := entry.Info()
				if err == nil {
					totalSize += fInfo.Size()
					fileExt := filepath.Ext(entry.Name())
					fileType := utils.DetectFileType(entry.Name())
					fileTypes[fileType]++

					// Adicionar informa√ß√µes do arquivo, incluindo a extens√£o
					builder.WriteString(fmt.Sprintf("%s (%s, %.1f KB, %s)\n",
						entry.Name(), fileExt, float64(fInfo.Size())/1024, fileType))
				} else {
					builder.WriteString(entry.Name() + "\n")
				}
			}
		}
		return nil
	}

	err = buildTree(path, "", 0)
	if err != nil {
		return "", fmt.Errorf("erro ao construir √°rvore de diret√≥rios: %w", err)
	}

	// Adicionar estat√≠sticas
	builder.WriteString("\nüìä ESTAT√çSTICAS:\n")
	builder.WriteString(fmt.Sprintf("Total de Diret√≥rios: %d\n", totalDirs))
	builder.WriteString(fmt.Sprintf("Total de Arquivos: %d\n", totalFiles))
	builder.WriteString(fmt.Sprintf("Tamanho Total: %.2f MB\n", float64(totalSize)/1024/1024))

	builder.WriteString("\nüîç TIPOS DE ARQUIVO:\n")
	for fileType, count := range fileTypes {
		builder.WriteString(fmt.Sprintf("%s: %d arquivos\n", fileType, count))
	}

	return builder.String(), nil
}

// processDirectoryChunked processa um diret√≥rio e divide o conte√∫do em chunks
func (cli *ChatCLI) processDirectoryChunked(path string, tokenEstimator func(string) int, maxTokens int) ([]FileChunk, error) {
	path, err := utils.ExpandPath(path)
	if err != nil {
		return nil, fmt.Errorf("erro ao expandir o caminho: %w", err)
	}

	// Configurar op√ß√µes de processamento de diret√≥rio
	scanOptions := utils.DefaultDirectoryScanOptions(cli.logger)
	scanOptions.OnFileProcessed = func(info utils.FileInfo) {
		cli.animation.UpdateMessage(fmt.Sprintf("Processando %s", info.Path))
	}

	// Sem limite de tamanho, vamos coletar tudo e depois dividir
	files, err := utils.ProcessDirectory(path, scanOptions)
	if err != nil {
		return nil, err
	}

	if len(files) == 0 {
		return nil, fmt.Errorf("nenhum arquivo relevante encontrado em '%s'", path)
	}

	cli.animation.UpdateMessage(fmt.Sprintf("Analisando e dividindo projeto em chunks (%d arquivos encontrados)", len(files)))

	// Dividir os arquivos em chunks
	var chunks []FileChunk
	var currentChunk strings.Builder
	filesInCurrentChunk := []utils.FileInfo{}

	// Fun√ß√£o para finalizar o chunk atual
	finishCurrentChunk := func() {
		if currentChunk.Len() > 0 {
			formattedContent := utils.FormatDirectoryContent(filesInCurrentChunk, int64(currentChunk.Len()))
			chunks = append(chunks, FileChunk{
				Index:   len(chunks) + 1,
				Content: formattedContent,
			})

			// Resetar para o pr√≥ximo chunk
			currentChunk.Reset()
			filesInCurrentChunk = []utils.FileInfo{}
		}
	}

	// Processar cada arquivo
	for _, file := range files {
		// Estimar tokens do conte√∫do do arquivo
		fileTokens := tokenEstimator(file.Content)

		// Se o arquivo for maior que metade do limite, criar um chunk s√≥ para ele
		if fileTokens > maxTokens/2 {
			// Finalizar chunk anterior se existir
			finishCurrentChunk()

			// Criar um chunk separado s√≥ para este arquivo grande
			chunks = append(chunks, FileChunk{
				Index:   len(chunks) + 1,
				Content: utils.FormatDirectoryContent([]utils.FileInfo{file}, file.Size),
			})
			continue
		}

		// Verificar se adicionar este arquivo excederia o limite de tokens
		currentTokens := tokenEstimator(currentChunk.String())
		if currentTokens+fileTokens > maxTokens*3/4 {
			finishCurrentChunk()
		}

		// Adicionar arquivo ao chunk atual
		currentChunk.WriteString(file.Content)
		filesInCurrentChunk = append(filesInCurrentChunk, file)
	}

	// Finalizar o √∫ltimo chunk se necess√°rio
	finishCurrentChunk()

	// Atualizar total em cada chunk
	for i := range chunks {
		chunks[i].Total = len(chunks)
	}

	return chunks, nil
}

// handleNextChunk processa o pr√≥ximo chunk de arquivos com tratamento de falhas
func (cli *ChatCLI) handleNextChunk() bool {
	if len(cli.fileChunks) == 0 {
		if len(cli.failedChunks) > 0 {
			fmt.Printf("N√£o h√° mais chunks pendentes, mas existem %d chunks com falha. Use /retry para retentar o √∫ltimo chunk com falha ou /retryall para retentar todos.\n", len(cli.failedChunks))
		} else {
			fmt.Println("N√£o h√° mais chunks de arquivos dispon√≠veis.")
		}
		return false
	}

	// Obter o pr√≥ximo chunk
	nextChunk := cli.fileChunks[0]

	// N√£o remova o chunk da fila at√© que tenhamos sucesso
	totalChunks := nextChunk.Total
	currentChunk := nextChunk.Index
	remainingChunks := len(cli.fileChunks) - 1

	fmt.Printf("Enviando chunk %d de %d... (%d restantes ap√≥s este)\n",
		currentChunk, totalChunks, remainingChunks)

	// Adicionar resumo de progresso
	progressInfo := fmt.Sprintf("üìä PROGRESSO: Chunk %d/%d\n"+
		"=============================\n"+
		"‚ñ∂Ô∏è %d chunks j√° processados\n"+
		"‚ñ∂Ô∏è %d chunks pendentes\n"+
		"‚ñ∂Ô∏è %d chunks com falha\n"+
		"‚ñ∂Ô∏è Use '/nextchunk' para avan√ßar ou '/retry' se ocorrer falha\n\n"+
		"=============================\n\n",
		currentChunk, totalChunks, currentChunk-1, remainingChunks, len(cli.failedChunks))

	// Preparar a mensagem
	prompt := fmt.Sprintf("Este √© o chunk %d/%d do c√≥digo que solicitei anteriormente. Por favor continue a an√°lise:",
		currentChunk, totalChunks)

	// Adicionar a mensagem ao hist√≥rico
	cli.history = append(cli.history, models.Message{
		Role:    "user",
		Content: prompt + "\n\n" + progressInfo + nextChunk.Content,
	})

	// Mostrar anima√ß√£o "Pensando..."
	cli.animation.ShowThinkingAnimation(cli.Client.GetModelName())

	// Criar contexto com timeout
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer cancel()

	// Enviar o prompt para o LLM
	aiResponse, err := cli.Client.SendPrompt(ctx, prompt+"\n\n"+nextChunk.Content, cli.history, 0)

	// Parar a anima√ß√£o
	cli.animation.StopThinkingAnimation()

	if err != nil {
		cli.logger.Error("Erro ao processar chunk com a LLM",
			zap.Int("chunkIndex", nextChunk.Index),
			zap.Int("totalChunks", nextChunk.Total),
			zap.Error(err))

		// Armazenar o chunk que falhou
		cli.lastFailedChunk = &cli.fileChunks[0]
		cli.failedChunks = append(cli.failedChunks, cli.fileChunks[0])

		// Remover da fila principal
		cli.fileChunks = cli.fileChunks[1:]

		// Informar ao usu√°rio
		fmt.Printf("\n‚ö†Ô∏è Erro ao processar o chunk %d/%d: %s\n",
			currentChunk, totalChunks, err.Error())
		fmt.Println("O chunk foi movido para a fila de chunks com falha.")
		fmt.Println("Use /retry para tentar novamente este chunk ou /nextchunk para continuar com o pr√≥ximo.")

		return false
	}

	// Se chegou aqui, o processamento foi bem-sucedido

	// Adicionar a resposta ao hist√≥rico
	cli.history = append(cli.history, models.Message{
		Role:    "assistant",
		Content: aiResponse,
	})

	// Renderizar e mostrar a resposta
	renderedResponse := cli.renderMarkdown(aiResponse)
	cli.typewriterEffect(fmt.Sprintf("\n%s:\n%s\n", cli.Client.GetModelName(), renderedResponse), 2*time.Millisecond)

	// Remover o chunk da fila apenas ap√≥s processamento bem-sucedido
	cli.fileChunks = cli.fileChunks[1:]

	// Informar sobre chunks restantes
	if len(cli.fileChunks) > 0 || len(cli.failedChunks) > 0 {
		fmt.Printf("\nStatus dos chunks:\n")

		if len(cli.fileChunks) > 0 {
			fmt.Printf("- %d chunks pendentes. Use /nextchunk para continuar.\n", len(cli.fileChunks))
		}

		if len(cli.failedChunks) > 0 {
			fmt.Printf("- %d chunks com falha. Use /retry ou /retryall para reprocess√°-los.\n", len(cli.failedChunks))
		}
	} else {
		fmt.Println("\nTodos os chunks foram processados com sucesso.")
	}

	return false
}

// Novo m√©todo para reprocessar o √∫ltimo chunk que falhou
func (cli *ChatCLI) handleRetryLastChunk() bool {
	if cli.lastFailedChunk == nil || len(cli.failedChunks) == 0 {
		fmt.Println("N√£o h√° chunks com falha para reprocessar.")
		return false
	}

	// Obter o √∫ltimo chunk com falha
	lastFailedIndex := len(cli.failedChunks) - 1
	chunk := cli.failedChunks[lastFailedIndex]

	// Remover da lista de falhas
	cli.failedChunks = cli.failedChunks[:lastFailedIndex]

	fmt.Printf("Retentando chunk %d/%d que falhou anteriormente...\n", chunk.Index, chunk.Total)

	// Preparar resumo de progresso
	progressInfo := fmt.Sprintf("üìä NOVA TENTATIVA: Chunk %d/%d\n"+
		"=============================\n"+
		"‚ñ∂Ô∏è Retentando chunk que falhou anteriormente\n"+
		"‚ñ∂Ô∏è %d chunks pendentes\n"+
		"‚ñ∂Ô∏è %d outros chunks com falha\n"+
		"=============================\n\n",
		chunk.Index, chunk.Total, len(cli.fileChunks), len(cli.failedChunks))

	// Preparar a mensagem
	prompt := fmt.Sprintf("Este √© o chunk %d/%d do c√≥digo (nova tentativa ap√≥s falha). Por favor continue a an√°lise:",
		chunk.Index, chunk.Total)

	// Adicionar a mensagem ao hist√≥rico
	cli.history = append(cli.history, models.Message{
		Role:    "user",
		Content: prompt + "\n\n" + progressInfo + chunk.Content,
	})

	// Mostrar anima√ß√£o "Pensando..."
	cli.animation.ShowThinkingAnimation(cli.Client.GetModelName())

	// Criar contexto com timeout
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer cancel()

	// Enviar o prompt para o LLM
	aiResponse, err := cli.Client.SendPrompt(ctx, prompt+"\n\n"+chunk.Content, cli.history, 0)

	// Parar a anima√ß√£o
	cli.animation.StopThinkingAnimation()

	if err != nil {
		cli.logger.Error("Erro ao reprocessar chunk com a LLM",
			zap.Int("chunkIndex", chunk.Index),
			zap.Int("totalChunks", chunk.Total),
			zap.Error(err))

		// Devolver o chunk para a lista de falhas
		cli.failedChunks = append(cli.failedChunks, chunk)

		// Informar ao usu√°rio
		fmt.Printf("\n‚ö†Ô∏è Erro ao reprocessar o chunk %d/%d: %s\n",
			chunk.Index, chunk.Total, err.Error())
		fmt.Println("O chunk permanece na fila de chunks com falha.")

		return false
	}

	// Adicionar a resposta ao hist√≥rico
	cli.history = append(cli.history, models.Message{
		Role:    "assistant",
		Content: aiResponse,
	})

	// Renderizar e mostrar a resposta
	renderedResponse := cli.renderMarkdown(aiResponse)
	cli.typewriterEffect(fmt.Sprintf("\n%s:\n%s\n", cli.Client.GetModelName(), renderedResponse), 2*time.Millisecond)

	// Atualizar o lastFailedChunk
	if len(cli.failedChunks) > 0 {
		lastIndex := len(cli.failedChunks) - 1
		cli.lastFailedChunk = &cli.failedChunks[lastIndex]
	} else {
		cli.lastFailedChunk = nil
	}

	fmt.Println("\nChunk reprocessado com sucesso!")

	// Informar sobre o status dos chunks
	cli.printChunkStatus()

	return false
}

// M√©todo para reprocessar todos os chunks com falha
func (cli *ChatCLI) handleRetryAllChunks() bool {
	if len(cli.failedChunks) == 0 {
		fmt.Println("N√£o h√° chunks com falha para reprocessar.")
		return false
	}

	fmt.Printf("Retentando todos os %d chunks com falha...\n", len(cli.failedChunks))

	// Mover todos os chunks com falha para a fila de chunks pendentes
	cli.fileChunks = append(cli.failedChunks, cli.fileChunks...)
	cli.failedChunks = []FileChunk{}
	cli.lastFailedChunk = nil

	// Iniciar o processamento do primeiro chunk
	return cli.handleNextChunk()
}

// M√©todo para pular explicitamente um chunk
func (cli *ChatCLI) handleSkipChunk() bool {
	if len(cli.fileChunks) == 0 {
		fmt.Println("N√£o h√° chunks pendentes para pular.")
		return false
	}

	skippedChunk := cli.fileChunks[0]
	cli.fileChunks = cli.fileChunks[1:]

	fmt.Printf("Pulando chunk %d/%d...\n", skippedChunk.Index, skippedChunk.Total)

	// Informar sobre o status dos chunks
	cli.printChunkStatus()

	return false
}

// M√©todo auxiliar para imprimir o status dos chunks
func (cli *ChatCLI) printChunkStatus() {
	fmt.Printf("\nStatus dos chunks:\n")

	if len(cli.fileChunks) > 0 {
		fmt.Printf("- %d chunks pendentes. Use /nextchunk para continuar.\n", len(cli.fileChunks))
	} else {
		fmt.Println("- N√£o h√° chunks pendentes.")
	}

	if len(cli.failedChunks) > 0 {
		fmt.Printf("- %d chunks com falha. Use /retry ou /retryall para reprocess√°-los.\n", len(cli.failedChunks))
	} else {
		fmt.Println("- N√£o h√° chunks com falha.")
	}
}

// extractUserQuery extrai a consulta do usu√°rio do input
func extractUserQuery(input string) string {
	// Remover o comando @file e qualquer op√ß√£o
	cleaned := removeAllFileCommands(input)
	return strings.TrimSpace(cleaned)
}

// processDirectorySmart processa um diret√≥rio e seleciona partes relevantes para a consulta
func (cli *ChatCLI) processDirectorySmart(path string, query string, tokenEstimator func(string) int, maxTokens int) (string, error) {
	path, err := utils.ExpandPath(path)
	if err != nil {
		return "", fmt.Errorf("erro ao expandir o caminho: %w", err)
	}

	// Se a consulta estiver vazia, usar o modo de resumo
	if query == "" {
		return cli.processDirectorySummary(path, tokenEstimator, maxTokens)
	}

	// Configurar op√ß√µes de processamento de diret√≥rio
	scanOptions := utils.DefaultDirectoryScanOptions(cli.logger)
	scanOptions.OnFileProcessed = func(info utils.FileInfo) {
		cli.animation.UpdateMessage(fmt.Sprintf("Analisando %s", info.Path))
	}

	files, err := utils.ProcessDirectory(path, scanOptions)
	if err != nil {
		return "", err
	}

	if len(files) == 0 {
		return "", fmt.Errorf("nenhum arquivo relevante encontrado em '%s'", path)
	}

	// Avaliar relev√¢ncia de cada arquivo para a consulta
	type ScoredFile struct {
		File  utils.FileInfo
		Score float64
	}

	var scoredFiles []ScoredFile

	// Termos importantes da consulta
	queryTerms := strings.Fields(strings.ToLower(query))

	for _, file := range files {
		// C√°lculo simples de relev√¢ncia baseado em correspond√™ncia de palavras-chave
		fileContent := strings.ToLower(file.Content)
		fileName := strings.ToLower(filepath.Base(file.Path))

		var score float64

		// Pontua√ß√£o por nome de arquivo
		for _, term := range queryTerms {
			if strings.Contains(fileName, term) {
				score += 5.0 // Maior peso para correspond√™ncia no nome
			}
		}

		// Pontua√ß√£o por conte√∫do
		for _, term := range queryTerms {
			count := strings.Count(fileContent, term)
			score += float64(count) * 0.5
		}

		// Normalizar pela extens√£o do arquivo (favorecendo arquivos de c√≥digo)
		ext := filepath.Ext(file.Path)
		if ext == ".go" || ext == ".java" || ext == ".py" || ext == ".js" || ext == ".ts" {
			score *= 1.2 // Bonus para arquivos de c√≥digo
		}

		// Penalizar arquivos muito grandes
		if file.Size > 1024*50 { // Maior que 50KB
			score *= 0.9
		}

		scoredFiles = append(scoredFiles, ScoredFile{File: file, Score: score})
	}

	// Ordenar arquivos por relev√¢ncia
	sort.Slice(scoredFiles, func(i, j int) bool {
		return scoredFiles[i].Score > scoredFiles[j].Score
	})

	// Selecionar os arquivos mais relevantes at√© atingir o limite de tokens
	var selectedFiles []utils.FileInfo
	var currentTokens int
	var builder strings.Builder

	builder.WriteString(fmt.Sprintf("üìÅ ARQUIVOS MAIS RELEVANTES PARA: \"%s\"\n\n", query))

	for _, scored := range scoredFiles {
		fileTokens := tokenEstimator(scored.File.Content)

		if currentTokens+fileTokens > maxTokens*3/4 {
			// Verificar se podemos incluir pelo menos um arquivo
			if len(selectedFiles) == 0 && fileTokens < maxTokens*3/4 {
				selectedFiles = append(selectedFiles, scored.File)
				currentTokens += fileTokens
				builder.WriteString(fmt.Sprintf("üìÑ %s (Pontua√ß√£o de relev√¢ncia: %.2f)\n",
					scored.File.Path, scored.Score))
			} else {
				break
			}
		} else {
			selectedFiles = append(selectedFiles, scored.File)
			currentTokens += fileTokens
			builder.WriteString(fmt.Sprintf("üìÑ %s (Pontua√ß√£o de relev√¢ncia: %.2f)\n",
				scored.File.Path, scored.Score))
		}
	}

	builder.WriteString(fmt.Sprintf("\nüîç Foram selecionados %d/%d arquivos mais relevantes para sua consulta.\n\n",
		len(selectedFiles), len(files)))

	// Se n√£o houver arquivos relevantes, retornar resumo
	if len(selectedFiles) == 0 {
		builder.WriteString("Nenhum arquivo teve pontua√ß√£o suficiente. Aqui est√° um resumo estrutural:\n\n")
		summary, err := cli.processDirectorySummary(path, tokenEstimator, maxTokens)
		if err != nil {
			return builder.String(), nil
		}
		builder.WriteString(summary)
		return builder.String(), nil
	}

	// Formatar o conte√∫do selecionado
	formattedContent := utils.FormatDirectoryContent(selectedFiles, int64(currentTokens))
	builder.WriteString(formattedContent)

	return builder.String(), nil
}

// removeAllFileCommands remove todos os comandos @file da entrada do usu√°rio
func removeAllFileCommands(input string) string {
	// Usa uma regex similar √† de extra√ß√£o para encontrar e remover todos os blocos @file
	re := regexp.MustCompile(`@file((?:\s+--\w+(?:(?:=|\s+)\S+)?)*\s+[\w~/.-]+/?[\w.-]*)`)
	cleaned := re.ReplaceAllString(input, "")

	// Limpa espa√ßos em branco extras que podem ter sido deixados para tr√°s
	return strings.TrimSpace(regexp.MustCompile(`\s+`).ReplaceAllString(cleaned, " "))
}

// filterEmptyLines remove linhas vazias
func filterEmptyLines(lines []string) []string {
	var filtered []string
	for _, line := range lines {
		if strings.TrimSpace(line) != "" {
			filtered = append(filtered, line)
		}
	}
	return filtered
}

// executeDirectCommand executa um comando diretamente no sistema
func (cli *ChatCLI) executeDirectCommand(command string) {
	fmt.Println("Executando comando:", command)

	// Verificar se o comando √© interativo
	isInteractive := false
	if strings.HasPrefix(command, "-i ") || strings.HasPrefix(command, "--interactive ") {
		isInteractive = true
		// Remover a flag do comando
		command = strings.TrimPrefix(command, "-i ")
		command = strings.TrimPrefix(command, "--interactive ")
	}

	// Verificar se o comando cont√©m a flag --send-ai e pipe |
	sendToAI := false
	var aiContext string
	if strings.Contains(command, "-ai") {
		sendToAI = true
		// Remover a flag do comando
		command = strings.Replace(command, "-ai", "", 1)
	}

	// Verificar se h√° um maior > no comando
	if strings.Contains(command, ">") {
		parts := strings.Split(command, ">")
		command = strings.TrimSpace(parts[0])
		aiContext = strings.TrimSpace(parts[1])
	}

	// Obter o shell do usu√°rio
	userShell := utils.GetUserShell()
	shellPath, err := exec.LookPath(userShell)
	if err != nil {
		cli.logger.Error("Erro ao localizar o shell", zap.Error(err))
		fmt.Println("Erro ao localizar o shell:", err)
		return
	}

	// Obter o caminho do arquivo de configura√ß√£o do shell
	shellConfigPath := utils.GetShellConfigFilePath(userShell)
	if shellConfigPath == "" {
		fmt.Println("N√£o foi poss√≠vel determinar o arquivo de configura√ß√£o para o shell:", userShell)
		return
	}

	// Construir o comando para carregar o arquivo de configura√ß√£o e executar o comando do usu√°rio
	shellCommand := fmt.Sprintf("source %s && %s", shellConfigPath, command)

	cmd := exec.Command(shellPath, "-c", shellCommand)

	if isInteractive {
		fmt.Println("Aviso: Executando comando interativo. O controle ser√° devolvido ao ChatCLI ao final.")
		cmd.Stdin = os.Stdin
		cmd.Stdout = os.Stdout
		cmd.Stderr = os.Stderr
		err = cmd.Run()
		if err != nil {
			fmt.Println("Erro ao executar comando:", err)
		}
		// Informar que a sa√≠da n√£o foi capturada
		fmt.Println("A sa√≠da do comando n√£o p√¥de ser capturada para o hist√≥rico.")
		// Armazenar apenas o comando no hist√≥rico
		cli.history = append(cli.history, models.Message{
			Role:    "system",
			Content: fmt.Sprintf("Comando executado: %s", command),
		})
		cli.lastCommandOutput = ""
	} else {
		// Capturar a sa√≠da do comando
		outputRaw, err := cmd.CombinedOutput()
		safeOutput := utils.SanitizeSensitiveText(string(outputRaw))
		safeCmd := utils.SanitizeSensitiveText(command)

		// Exibir a sa√≠da
		fmt.Println("Sa√≠da do comando:\n\n", safeOutput)

		if err != nil {
			fmt.Println("Erro ao executar comando:", err)
		}

		// Armazenar a sa√≠da no hist√≥rico
		cli.history = append(cli.history, models.Message{
			Role:    "system",
			Content: fmt.Sprintf("Comando: %s\nSa√≠da:\n%s", safeCmd, safeOutput),
		})
		cli.lastCommandOutput = safeOutput

		// se a flag --ai foi passada enviar o output para a IA
		if sendToAI {
			cli.sendOutputToAI(cli.lastCommandOutput, aiContext)
		}
	}

	// Adicionar o comando ao hist√≥rico do liner para persistir em .chatcli_history
	//cli.line.AppendHistory(fmt.Sprintf("@command %s", command))
}

// sendOutputToAI envia o output do comando para a IA com o contexto adicional
func (cli *ChatCLI) sendOutputToAI(output string, aiContext string) {
	fmt.Println("Enviando s√°ida do comando para a IA...")

	safeOutput := utils.SanitizeSensitiveText(output)
	safeContext := utils.SanitizeSensitiveText(aiContext)

	// Adicionar o output do comando ao hist√≥rico como mensagem do usu√°rio
	cli.history = append(cli.history, models.Message{
		Role:    "user",
		Content: fmt.Sprintf("Sa√≠da do comando:\n%s\n\nContexto: %s", safeOutput, safeContext),
	})
	// Exibir mensagem "Pensando..." com anima√ß√£o
	cli.animation.ShowThinkingAnimation(cli.Client.GetModelName())

	//Criar um contexto com timeout
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Minute)
	defer cancel()

	//Enviar o output e o contexto para a IA
	aiResponse, err := cli.Client.SendPrompt(ctx, fmt.Sprintf("Sa√≠da do comando:\n%s\n\nContexto: %s", safeOutput, safeContext), cli.history, 0)

	//parar a anima√ß√£o
	cli.animation.StopThinkingAnimation()

	if err != nil {
		cli.logger.Error("Erro do LLM", zap.Error(err))
		fmt.Println("Ocorreu um erro ao processar a requisi√ß√£o.")
		return
	}

	// Adicionar a resposta da IA ao hist√≥rico
	cli.history = append(cli.history, models.Message{
		Role:    "assistant",
		Content: aiResponse,
	})

	// Renderizar a resposta da IA
	renderResponse := cli.renderMarkdown(aiResponse)

	// Exibir a resposta da IA com efeito de digita√ß√£o
	cli.typewriterEffect(fmt.Sprintf("\n%s:\n%s\n", cli.Client.GetModelName(), renderResponse), 2*time.Millisecond)
}

// completer (vers√£o final, flex√≠vel para @comandos, restritiva para /comandos)
func (cli *ChatCLI) completer(d prompt.Document) []prompt.Suggest {
	// 1. Lidar com estados especiais primeiro (como a troca de provedor)
	if cli.interactionState == StateSwitchingProvider {
		providers := cli.manager.GetAvailableProviders()
		s := make([]prompt.Suggest, len(providers))
		for i, p := range providers {
			s[i] = prompt.Suggest{Text: strconv.Itoa(i + 1), Description: p}
		}
		return prompt.FilterHasPrefix(s, d.GetWordBeforeCursor(), true)
	}

	// 2. Extrair informa√ß√µes do documento atual
	lineBeforeCursor := d.TextBeforeCursor()
	wordBeforeCursor := d.GetWordBeforeCursor()
	args := strings.Fields(lineBeforeCursor)

	// --- L√≥gica de Autocomplete Contextual ---

	// 3. Autocomplete para argumentos de comandos @ (como caminhos para @file)
	if len(args) > 0 {
		var previousWord string
		if strings.HasSuffix(lineBeforeCursor, " ") {
			previousWord = args[len(args)-1]
		} else if len(args) > 1 {
			previousWord = args[len(args)-2]
		}

		// Apenas autocompletar caminhos se a palavra atual N√ÉO for uma flag
		if previousWord == "@file" && !strings.HasPrefix(wordBeforeCursor, "-") {
			return cli.filePathCompleter(wordBeforeCursor)
		}

		if previousWord == "@command" && !strings.HasPrefix(wordBeforeCursor, "-") {
			suggestions := cli.systemCommandCompleter(wordBeforeCursor)
			suggestions = append(suggestions, cli.filePathCompleter(wordBeforeCursor)...)
			return suggestions
		}
	}

	// 4. Autocomplete para iniciar comandos
	if !strings.Contains(lineBeforeCursor, " ") {
		if strings.HasPrefix(wordBeforeCursor, "/") {
			return prompt.FilterHasPrefix(cli.getInternalCommands(), wordBeforeCursor, true)
		}
	}

	if strings.HasPrefix(wordBeforeCursor, "@") {
		return prompt.FilterHasPrefix(cli.getContextCommands(), wordBeforeCursor, true)
	}

	// 5. Sugest√µes de flags e valores (L√ìGICA FINAL CORRIGIDA)
	if len(args) > 1 {
		command := args[0]
		prevWord := args[len(args)-1]
		if !strings.HasSuffix(lineBeforeCursor, " ") && len(args) > 1 {
			prevWord = args[len(args)-2]
		}
		currWord := d.GetWordBeforeCursor()

		if flagsForCommand, commandExists := commandFlags[command]; commandExists {
			// Cen√°rio 1: O usu√°rio digitou uma flag (ex: "--mode ") e agora quer ver os valores.
			if values, flagHasValues := flagsForCommand[prevWord]; flagHasValues && len(values) > 0 {
				return prompt.FilterHasPrefix(values, currWord, true)
			}

			// Cen√°rio 2: O usu√°rio est√° digitando uma flag (ex: "--m").
			if strings.HasPrefix(currWord, "-") {
				var flagSuggests []prompt.Suggest
				for flag, values := range flagsForCommand {
					var desc string
					// 1. Primeiro, procurar por descri√ß√µes personalizadas
					if flag == "--mode" {
						desc = "Define o modo de processamento de arquivos (full, summary, chunked, smart)"
					} else if flag == "--model" {
						desc = "Troque o modelo (Runtime) baseado no provedor atual (grpt-5, grok-4, etc.)"
					} else if flag == "--max-tokens" {
						desc = "Define o m√°ximo de tokens para as pr√≥ximas respostas (0 para padr√£o)"
					} else if flag == "--slugname" {
						desc = "Altera o Slug em tempo de execu√ß√£o (Apenas para STACKSPOT)"
					} else if flag == "--tenantname" {
						desc = "Altera o Tenant em tempo de execu√ß√£o (Apenas para STACKSPOT)"
					} else if flag == "-i" {
						desc = "Ideal para comandos interativos evitando sensa√ß√£o de bloqueio do terminal"
					} else if flag == "--ai" {
						desc = "Envia a sa√≠da do comando direto para a IA analisar, para contexto adicional digite ( @command --ai <comando> > <contexto>)"
					} else {
						// 2. Se n√£o houver descri√ß√£o personalizada, criar uma gen√©rica
						desc = fmt.Sprintf("Op√ß√£o para %s", command)
						if len(values) > 0 {
							desc += " (valores: " + strings.Join(extractTexts(values), ", ") + ")"
						}
					}
					flagSuggests = append(flagSuggests, prompt.Suggest{Text: flag, Description: desc})
				}
				return prompt.FilterHasPrefix(flagSuggests, currWord, true)
			}
		}
	}

	// 6. Se nenhum dos casos acima se aplicar, n√£o sugira nada.
	return []prompt.Suggest{}
}

// Helper para extrair s√≥ os Texts de um []Suggest (para descri√ß√µes de flags)
func extractTexts(suggests []prompt.Suggest) []string {
	texts := make([]string, len(suggests))
	for i, s := range suggests {
		texts[i] = s.Text
	}
	return texts
}

func (cli *ChatCLI) getInternalCommands() []prompt.Suggest {
	return []prompt.Suggest{
		{Text: "/exit", Description: "Sair do ChatCLI"},
		{Text: "/quit", Description: "Alias de /exit - Sair do ChatCLI"},
		{Text: "/switch", Description: "Trocar o provedor de LLM, seguido por --model troca o modelo"},
		{Text: "/help", Description: "Mostrar ajuda"},
		{Text: "/reload", Description: "Recarregar configura√ß√µes do .env"},
		{Text: "/config", Description: "Mostrar configura√ß√£o atual"},
		{Text: "/status", Description: "Alias de /config - Mostrar configura√ß√£o atual"},
		{Text: "/agent", Description: "Iniciar modo agente para executar tarefas"},
		{Text: "/run", Description: "Alias para /agent - Iniciar modo agente para executar tarefas"},
		{Text: "/newsession", Description: "Iniciar uma nova sess√£o de conversa"},
		{Text: "/version", Description: "Verificar a vers√£o do ChatCLI"},
		{Text: "/nextchunk", Description: "Carregar o pr√≥ximo chunk de arquivo"},
		{Text: "/retry", Description: "Tentar novamente o √∫ltimo chunk que falhou"},
		{Text: "/retryall", Description: "Tentar novamente todos os chunks que falharam"},
		{Text: "/skipchunk", Description: "Pular um chunk de arquivo"},
		{Text: "/session", Description: "Gerencia as sess√µes, new, save, list, load, delete"},
	}
}

// getContextCommands retorna a lista de sugest√µes para comandos com @
func (cli *ChatCLI) getContextCommands() []prompt.Suggest {
	return []prompt.Suggest{
		{Text: "@history", Description: "Adicionar hist√≥rico do shell ao contexto"},
		{Text: "@git", Description: "Adicionar informa√ß√µes do Git ao contexto"},
		{Text: "@env", Description: "Adicionar vari√°veis de ambiente ao contexto"},
		{Text: "@file", Description: "Adicionar conte√∫do de um arquivo ou diret√≥rio"},
		{Text: "@command", Description: "Executar um comando do sistema e usar a sa√≠da"},
	}
}

// filePathCompleter √© uma fun√ß√£o dedicada para autocompletar caminhos de arquivo
func (cli *ChatCLI) filePathCompleter(prefix string) []prompt.Suggest {
	var suggestions []prompt.Suggest
	completions := cli.completeFilePath(prefix)
	for _, c := range completions {
		suggestions = append(suggestions, prompt.Suggest{Text: c})
	}
	return suggestions
}

// systemCommandCompleter √© uma fun√ß√£o dedicada para autocompletar comandos do sistema
func (cli *ChatCLI) systemCommandCompleter(prefix string) []prompt.Suggest {
	var suggestions []prompt.Suggest
	completions := cli.completeSystemCommands(prefix)
	for _, c := range completions {
		suggestions = append(suggestions, prompt.Suggest{Text: c})
	}
	return suggestions
}

// completeFilePath autocompleta caminhos de arquivos
func (cli *ChatCLI) completeFilePath(prefix string) []string {
	var completions []string

	dir, filePrefix := filepath.Split(prefix)
	if dir == "" {
		dir = "."
	}

	// Expandir "~" para o diret√≥rio home
	dir = os.ExpandEnv(dir)
	if strings.HasPrefix(dir, "~") {
		homeDir, err := os.UserHomeDir()
		if err == nil {
			dir = filepath.Join(homeDir, dir[1:])
		}
	}

	files, err := os.ReadDir(dir)
	if err != nil {
		return completions
	}

	for _, entry := range files {
		name := entry.Name()
		if strings.HasPrefix(name, filePrefix) {
			path := filepath.Join(dir, name)
			if entry.IsDir() {
				path += string(os.PathSeparator)
			}
			completions = append(completions, path)
		}
	}

	return completions
}

// completeSystemCommands autocompleta comandos do sistema
func (cli *ChatCLI) completeSystemCommands(prefix string) []string {
	var completions []string

	// Obter o PATH do sistema
	pathEnv := os.Getenv("PATH")
	paths := strings.Split(pathEnv, string(os.PathListSeparator))

	seen := make(map[string]bool)

	for _, pathDir := range paths {
		files, err := os.ReadDir(pathDir)
		if err != nil {
			continue
		}

		for _, file := range files {
			name := file.Name()
			if strings.HasPrefix(name, prefix) && !seen[name] {
				seen[name] = true
				completions = append(completions, name)
			}
		}
	}

	return completions
}

// renderMarkdown renderiza o texto em Markdown
func (cli *ChatCLI) renderMarkdown(input string) string {
	// Ajustar a largura para o tamanho do terminal
	//width, _, err := utils.GetTerminalSize()
	//if err != nil || width <= 0 {
	//	width = 80 // valor padr√£o
	//}
	renderer, _ := glamour.NewTermRenderer(
		glamour.WithAutoStyle(),
		glamour.WithWordWrap(0),
	)
	out, err := renderer.Render(input)
	if err != nil {
		return input
	}
	return out
}

// typewriterEffect exibe o texto com efeito de m√°quina de escrever
func (cli *ChatCLI) typewriterEffect(text string, delay time.Duration) {
	reader := strings.NewReader(text)
	inEscapeSequence := false

	for {
		char, _, err := reader.ReadRune()
		if err != nil {
			break // Fim do texto
		}

		// Verifica se √© o in√≠cio de uma sequ√™ncia de escape
		if char == '\033' {
			inEscapeSequence = true
		}

		fmt.Printf("%c", char)
		os.Stdout.Sync()

		// Verifica o final da sequ√™ncia de escape
		if inEscapeSequence {
			if char == 'm' {
				inEscapeSequence = false
			}
			continue // N√£o aplica delay dentro da sequ√™ncia de escape
		}

		time.Sleep(delay) // Ajuste o delay conforme desejado
	}
}

// presence retorna "[SET]" ou "[NOT SET]" para uma env sens√≠vel
func presence(v string) string {
	if strings.TrimSpace(v) == "" {
		return "[NOT SET]"
	}
	return "[SET]"
}

// getEnvFilePath retorna o caminho do arquivo .env configurado (expandido).
func (cli *ChatCLI) getEnvFilePath() string {
	envFilePath := os.Getenv("CHATCLI_DOTENV")
	if envFilePath == "" {
		envFilePath = ".env"
	}
	expanded, err := utils.ExpandPath(envFilePath)
	if err != nil {
		cli.logger.Warn("N√£o foi poss√≠vel expandir o caminho do .env", zap.Error(err))
		return envFilePath // Retorna o original se falhar
	}
	return expanded
}

func (cli *ChatCLI) showConfig() {
	// Helper para formatar linhas com alinhamento e cores (similar ao /help)
	printItem := func(key, value string) {
		keyColor := ColorCyan
		valueColor := ColorGray
		if strings.Contains(value, "[SET]") {
			valueColor = ColorGreen
		} else if strings.Contains(value, "[NOT SET]") {
			valueColor = ColorYellow
		}
		fmt.Printf("    %s    %s\n", colorize(fmt.Sprintf("%-25s", key+":"), keyColor), colorize(value, valueColor))
	}

	// Cabe√ßalho
	fmt.Println("\n" + colorize(ColorBold, "Configura√ß√£o Atual do ChatCLI"))
	fmt.Println(colorize("Aqui est√° um resumo das configura√ß√µes em tempo de execu√ß√£o, provedores e vari√°veis de ambiente.", ColorGray))

	// --- Geral ---
	fmt.Printf("\n  %s\n", colorize("Configura√ß√µes Gerais", ColorLime))
	printItem("Arquivo .env", cli.getEnvFilePath())
	printItem("Ambiente (ENV)", os.Getenv("ENV"))
	printItem("N√≠vel de Log (LOG_LEVEL)", os.Getenv("LOG_LEVEL"))
	printItem("Arquivo de Log (LOG_FILE)", os.Getenv("LOG_FILE"))
	printItem("Tamanho M√°x. Log (LOG_MAX_SIZE)", os.Getenv("LOG_MAX_SIZE"))
	printItem("Tamanho M√°x. Hist√≥rico (HISTORY_MAX_SIZE)", os.Getenv("HISTORY_MAX_SIZE"))

	// --- Provedor e Modelo Atuais ---
	fmt.Printf("\n  %s\n", colorize("Provedor e Modelo Atuais", ColorLime))
	printItem("Provedor (Runtime)", cli.Provider)
	printItem("Modelo (Runtime)", cli.Model)
	printItem("Nome do Modelo (Client)", cli.Client.GetModelName())
	printItem("API Preferida (Cat√°logo)", string(catalog.GetPreferredAPI(cli.Provider, cli.Model)))
	printItem("MaxTokens Efetivo", fmt.Sprintf("%d", cli.getMaxTokensForCurrentLLM()))

	// --- Overrides por ENV ---
	fmt.Printf("\n  %s\n", colorize("Overrides de MaxTokens por Provedor (ENV)", ColorLime))
	printItem("OPENAI_MAX_TOKENS", os.Getenv("OPENAI_MAX_TOKENS"))
	printItem("CLAUDEAI_MAX_TOKENS", os.Getenv("CLAUDEAI_MAX_TOKENS"))
	printItem("GOOGLEAI_MAX_TOKENS", os.Getenv("GOOGLEAI_MAX_TOKENS"))
	printItem("XAI_MAX_TOKENS", os.Getenv("XAI_MAX_TOKENS"))
	printItem("OLLAMA_MAX_TOKENS", os.Getenv("OLLAMA_MAX_TOKENS"))

	// --- Chaves Sens√≠veis (Presen√ßa Apenas) ---
	fmt.Printf("\n  %s\n", colorize("Chaves Sens√≠veis (Presen√ßa Apenas)", ColorLime))
	printItem("OPENAI_API_KEY", presence(os.Getenv("OPENAI_API_KEY")))
	printItem("CLAUDEAI_API_KEY", presence(os.Getenv("CLAUDEAI_API_KEY")))
	printItem("GOOGLEAI_API_KEY", presence(os.Getenv("GOOGLEAI_API_KEY")))
	printItem("XAI_API_KEY", presence(os.Getenv("XAI_API_KEY")))
	printItem("CLIENT_ID (StackSpot)", presence(os.Getenv("CLIENT_ID")))
	printItem("CLIENT_SECRET (StackSpot)", presence(os.Getenv("CLIENT_SECRET")))

	// --- Configura√ß√µes por Provedor ---
	fmt.Printf("\n  %s\n", colorize("Configura√ß√µes por Provedor", ColorLime))
	//if strings.ToUpper(cli.Provider) == "OPENAI" || strings.ToUpper(cli.Provider) == "OPENAI_ASSISTANT" {
	printItem("OPENAI_MODEL", os.Getenv("OPENAI_MODEL"))
	printItem("OPENAI_ASSISTANT_MODEL", os.Getenv("OPENAI_ASSISTANT_MODEL"))
	printItem("OPENAI_USE_RESPONSES", os.Getenv("OPENAI_USE_RESPONSES"))
	//}
	//if strings.ToUpper(cli.Provider) == "CLAUDEAI" {
	printItem("CLAUDEAI_MODEL", os.Getenv("CLAUDEAI_MODEL"))
	printItem("CLAUDEAI_API_VERSION", os.Getenv("CLAUDEAI_API_VERSION"))
	//}
	//if strings.ToUpper(cli.Provider) == "GOOGLEAI" {
	printItem("GOOGLEAI_MODEL", os.Getenv("GOOGLEAI_MODEL"))
	//}
	//if strings.ToUpper(cli.Provider) == "XAI" {
	printItem("XAI_MODEL", os.Getenv("XAI_MODEL"))
	printItem("OLLAMA_MODEL", os.Getenv("OLLAMA_MODEL"))
	printItem("OLLAMA_BASE_URL", utils.GetEnvOrDefault("OLLAMA_BASE_URL", config.OllamaDefaultBaseURL))
	//}
	if tm, ok := cli.manager.GetTokenManager(); ok {
		slug, tenant := tm.GetSlugAndTenantName()
		printItem("STACKSPOT: slugName", slug)
		printItem("STACKSPOT: tenantName", tenant)
	}

	// --- Provedores Dispon√≠veis ---
	fmt.Printf("\n  %s\n", colorize("Provedores Dispon√≠veis", ColorLime))
	providers := cli.manager.GetAvailableProviders()
	if len(providers) > 0 {
		for i, p := range providers {
			printItem(fmt.Sprintf("Provedor %d", i+1), p)
		}
	} else {
		printItem("Nenhum", "Configure as chaves de API no .env")
	}
}

// handleVersionCommand exibe informa√ß√µes detalhadas sobre a vers√£o atual
// do ChatCLI e verifica se h√° atualiza√ß√µes dispon√≠veis no GitHub.
//
// O comando mostra:
// - Vers√£o atual (tag ou hash do commit)
// - Hash do commit exato
// - Data e hora de build
// - Status de atualiza√ß√£o (verificando o GitHub quando poss√≠vel)
// handleVersionCommand exibe informa√ß√µes detalhadas sobre a vers√£o atual
// do ChatCLI e verifica se h√° atualiza√ß√µes dispon√≠veis no GitHub.
func (ch *CommandHandler) handleVersionCommand() {
	versionInfo := version.GetCurrentVersion()

	// Checagem com timeout
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()
	latest, hasUpdate, err := version.CheckLatestVersionWithContext(ctx)

	// Exibir as informa√ß√µes formatadas
	fmt.Println(version.FormatVersionInfo(versionInfo, latest, hasUpdate, err))
}

// RunAgentOnce executa o modo agente de forma n√£o-interativa (one-shot)
func (cli *ChatCLI) RunAgentOnce(ctx context.Context, input string, autoExecute bool) error {
	var query string
	if strings.HasPrefix(input, "/agent ") {
		query = strings.TrimPrefix(input, "/agent ")
	} else if strings.HasPrefix(input, "/run ") {
		query = strings.TrimPrefix(input, "/run ")
	} else {
		return fmt.Errorf("entrada inv√°lida para o modo agente one-shot: %s", input)
	}

	// Processar contextos especiais como @file, @git, etc.
	query, additionalContext := cli.processSpecialCommands(query)
	fullQuery := query
	if additionalContext != "" {
		fullQuery = query + "\n\nContexto adicional:\n" + additionalContext
	}

	// Assegurar que o modo agente est√° inicializado
	if cli.agentMode == nil {
		cli.agentMode = NewAgentMode(cli, cli.logger)
	}

	// Chama a nova fun√ß√£o n√£o-interativa do AgentMode
	return cli.agentMode.RunOnce(ctx, fullQuery, autoExecute)
}

func (cli *ChatCLI) handleSaveSession(name string) {
	if err := cli.sessionManager.SaveSession(name, cli.history); err != nil {
		fmt.Printf(" ‚ùå Erro ao salvar sess√£o: %v\n", err)
	} else {
		cli.currentSessionName = name
		fmt.Printf(" ‚úÖ Sess√£o '%s' salva com sucesso.\n", name)
	}
}

func (cli *ChatCLI) handleLoadSession(name string) {
	history, err := cli.sessionManager.LoadSession(name)
	if err != nil {
		fmt.Printf(" ‚ùå Erro ao carregar sess√£o: %v\n", err)
	} else {
		cli.history = history
		cli.currentSessionName = name
		fmt.Printf(" ‚úÖ Sess√£o '%s' carregada. A conversa anterior foi restaurada.\n", name)
	}
}

func (cli *ChatCLI) handleListSessions() {
	sessions, err := cli.sessionManager.ListSessions()
	if err != nil {
		fmt.Printf(" ‚ùå Erro ao listar sess√µes: %v\n", err)
		return
	}

	if len(sessions) == 0 {
		fmt.Println("Nenhuma sess√£o salva encontrada.")
		return
	}

	fmt.Println("Sess√µes salvas:")
	for _, session := range sessions {
		fmt.Printf("- %s\n", session)
	}
}

func (cli *ChatCLI) handleDeleteSession(name string) {
	if err := cli.sessionManager.DeleteSession(name); err != nil {
		fmt.Printf(" ‚ùå Erro ao deletar sess√£o: %v\n", err)
	} else {
		fmt.Printf(" ‚úÖ Sess√£o '%s' deletada com sucesso do disco.\n", name)
		// Se a sess√£o deletada era a que estava ativa...
		if cli.currentSessionName == name {
			// ...limpamos o hist√≥rico em mem√≥ria e resetamos o nome.
			cli.history = []models.Message{}
			cli.currentSessionName = ""
			fmt.Println("A sess√£o atual foi limpa. Voc√™ est√° em uma nova conversa.")
		}
	}
}
